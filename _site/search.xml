<?xml version="1.0" encoding="utf-8"?>
<search>
  
    <entry>
      <title><![CDATA[tf.estimator 사용하기 - 01. tf.data와 tf.record를 이용해 데이터 읽고 저장하기]]></title>
      <url>/tensorflow/deep%20learning/2019/02/13/tf.Estimator-%EC%82%AC%EC%9A%A9%ED%95%98%EA%B8%B0-01-%EB%8D%B0%EC%9D%B4%ED%84%B0-%EB%A7%8C%EB%93%A4%EA%B8%B0/</url>
      <content type="text"><![CDATA[tf.estimator 와 효과적인 Data 사용tf.estimator 는 Tensorflow의 High-level API로 평가, 학습, 예측과 서빙을 위한 모델 저장등을 편하게 수행할 수 있게 해줍니다. Estimators 를 사용하면서 (Estimator 외에도) Tensorflow 에서 데이터를 효과적으로 사용하기 위해서는 데이터를 여러 개의 직렬화된 데이터에 나눠 저장해 이용하는 것이 좋습니다. Tensorflow 에서는 TFRecord 를 통해 데이터를 직렬화하고 읽을 수 있습니다. 데이터를 직렬화하고 TFRecord 형식으로 저장하고 이를 읽기 위해 tf.Example 을 이용할 수 있습니다.여기서는 먼저 .txt 형식의 파일을 tf.data.TextLineDataset 을 이용해 읽어 오고, tf.Example 로 데이터를 직렬화한 뒤, TFRecord 형식으로 다시 저장하는 과정을 진행해봅니다.데이터 읽고 파싱하기먼저 일반적인 .txt 형식의 파일을 tf.data.TextLineDataset 으로 읽어오겠습니다. 예제 데이터로 NSMC (Naver Sentiment Movie Corpus) 를 이용합니다.tf.data 로 데이터를 읽는 것은 간단합니다. tf.data (여기서는 tf.data.TextLineDataset) 을 선언하고 한 번에 읽ㅇ어 올 배치 사이즈를 지정한 다음, iterator 를 생성합니다. 구체적인 코드는 아래와 같습니다.FEATURE_NAME = "review"hparams = tf.contrib.training.HParams(batchsize=1)def iterate_data(filename):    """    Read text file with tf.data.TextLineDataset.    """    def _decode_tsv(line):        parsed_line = tf.decode_csv(line, [[0], ["null"], [0]], field_delim="\t")        label = parsed_line[-1]        feature = parsed_line[1]        d = dict(zip([FEATURE_NAME], [feature])), label        return d    dataset = tf.data.TextLineDataset(filename).skip(1).map(_decode_tsv)    dataset = dataset.batch(hparams.batch_size)    return dataset.make_one_shot_iterator()Iterator 를 리턴하는 iterate_data(filename) 함수를 정의합니다. 이 함수는 내부에서 텍스트 파일 한 라인을 파싱할 _decode_tsv(line) 을 매핑합니다. tf.decode_csv(records, record_defaults) 의 record_defaults 는 한 라인 각 요소의 데이터 타입을 예시로 지정해줍니다. NSMC 데이터는 각 라인이 \t 으로 분리되어 있고, 각 원소는 id (int), 리뷰 (string), sentiment (int) 로 이루어져 있습니다. 이를 위해 record_defaults 를 [[0], ["null"], [0]] 으로 지정해 주었습니다. 한편, decode 함수는 (feature Dict, label) 튜플을 리턴합니다.이후 위의 함수를 이용해서 데이터를 불러오는 코드는 다음과 같습니다.iterator = iterate_data(TRAIN_PATH)next_elem = iterator.get_next()with tf.Session() as sess:    batch_features, batch_labels = sess.run(next_elem)데이터를 불러오면 다음과 같습니다.batch_features&gt;&gt;&gt;{'review': array([b'\xea\xb5\xb3 \xe3\x85\x8b'], dtype=object)}batch_labels&gt;&gt;&gt;array([1], dtype=int32)tf.data 로 데이터를 불러오면 텍스트는 utf-8로 인코딩 된 상태입니다. 우리가 읽을 수 있고, 여러 가지 전처리를 하기 위해서는 str.decode("utf-8") 로 디코딩 해줍니다. (Tensorflow 1.3 에서는 unicode 인코딩/디코딩 API가 제공됩니다.)batch_feature["review"][0].decode("utf-8")&gt;&gt;&gt;'굳 ㅋ'위에서는 예시로 들기 위해 첫 번째 배치만 얻고 반복문을 중지했지만 실제로는 while 문으로 데이터를 끝까지 불러옵니다. 유의할 점은, 데이터를 불러오는 .get_next() 를 반복문 안이 아니라 밖에서 선언한다는 점입니다. 실제로는 아래와 같은 형태로 데이터를 읽습니다.with tf.Session() as sess:    while True:        try:                        batch_features, batch_labels = sess.run(next_elem)        except tf.errors.OutOfRangeError:            breaktf.Example 로 Example 만들기그럼 데이터를 읽어올 수 있으니, TFRecord로 다시 저장하기 위해 Example 을 만들어 줍니다. tf.Example 을 이용합니다.      데이터 만들기    데이터 저장  tf.record 불러오기          tf.data.TfRecordDataset      tf.python_io.      ]]></content>
      <categories>
        
          <category> Tensorflow </category>
        
          <category> Deep Learning </category>
        
      </categories>
      <tags>
        
          <tag> deep learning </tag>
        
          <tag> tensorflow </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[[Review] You May Not Need Attention]]></title>
      <url>/nlp/deep%20learning/2018/11/16/You-May-Not-Need-Attention/</url>
      <content type="text"><![CDATA[Introduction최근 활발히 연구되고 있는 NMT 모델은 다음의 속성을 따른다.  Decoder는 source sequence representation에 대해 attention을 사용한다.  Encoder와 Decoder는 두 개의 다른 모듈이며, Encoder는 Decoder의 작동 전에 source sentence의 encoding을 마쳐야 한다.논문에서는 NMT 모델이 위의 속성 없이 얼마나 성능을 낼 수 있는지 확인한다. 논문은 기존의 Encoder-Decoder with attention 모델과 다른 구조로 NMT를 실험했고, 이 모델을 eager translation model이라고 부른다.이를 위해 논문에서는 Bahdanau et al. (2014)의 모델로 시작해 attention을 제거하고 Encoder와 Decoder를 하나의 간단한 모델로 합친다. 이 모델은 Zaremba et al. (2014)의 langaugae model을 닮았다. 이를 통해 논문의 모델은 source word가 입력되지마자 번역을 시작할 수 있다. Eager translation model은 일정 개수의 메모리만 사용하는데 이는 매 타임 스텝마다 직전 타임 스텝의 히든 스테이트만 이용하기 때문이다. 한편, 논문의 모델이 다른 모델과 가장 큰 차이점을 보이는 곳은 preprocessing이다.Data Preprocessing논문에서 소개하는 preprocessing의 핵심은 source/target 문장의 align (정렬)이다. 논문에서는 이를 위해 source/target sentence에 특정 속성을 요구한다.  source/target 문장의 대응 관계를 inferring 하는데, 이 때 각 target 단어는 최대 하나의 source 단어에 매칭된다. 그리고 정렬된 $(s_i, \boldsymbol{t}_j)$ 쌍이 $i \le j$ 를 만족하는 상태를 eager feasible 하다고 부른다.논문에서는 위의 속성을 만족시키기 위해 먼저 off-the-shelf 모델 (fast_align; Dyer at el., 2013) 을 사용해 source/target 문장 간의 대응을 예측한 뒤 최소한의 $\epsilon$ (empty) 토큰을 target 문장에 삽입한다. 이 $\epsilon$ 토큰은 학습과 예측 단계에서만 사용되고 실제 번역 문장을 만들어낼 때에는 제거된다.  fast_align 모델은 source/target 문장 간 대응의 maximum likelihood를 log-linear 속도로 찾아내는 간단하고 빠른 모델이다.예를 들어 source 문장 “El perro blanco” (“The dog white”)의 올바른 번역은 “The white dog.”이다. 영어 문장의 두 번째 단어 (white)와 스페인 문장의 세 번째 단어가 분명하게 대응 (정렬)된다는 것을 가정하면 시퀀스를 eager feasible하게 만들기 위해 논문의 모델은 target 문장을 “The $\epsilon$ white dog.” 으로 변환한다.논문에서 소개하는 eager feasible 알고리즘은 아래와 같다.  source 문장을 $s = \langle s_1, \ldots, s_m \rangle$ , target 문장을 $\boldsymbol{t} = \langle t_1, \ldots, t_n \rangle$ 이라고 했을 때 $\mathcal{A}$ 를 정렬 쌍 $(i, j)$ 의 집합으로 둔다. ($t_j$ 가 $s_i$ 에 대응된다.)  target 문장을 왼쪽에서 오른쪽으로 살펴본다. 현재 target 단어가 $t$ 이고 현재의 위치는 $j$ 로 가정한다. $t$ 가 source 단어 $s_i$ 와 정렬되고 (에 대응하고) $i \le j$ 이면 다음 target 단어로 넘어간다. (정렬된 상태이므로)      그렇지 않을 경우 해당 target 단어 바로 앞에 $\epsilon$ 토큰을 삽입한다. 토큰 삽입으로 $t$ 는 target 문장에서 $i$ 위치로 이동한다. 물론, 다른 target 단어들도 오른쪽으로 이동한다.    또한 위의 작업을 조금 더 간단하게 하기 위해 논문에서는 모든 target 문장 앞에 $b \in { 0, 1, \ldots, 5 }$ 개의 $\epsilon$ 토큰을 미리 더한 뒤 실험했다. 이는 모델이 번역 전에 더 많은 source 문장을 소비하도록 한다. 즉 예측 시에 모델이 $b$ 개의 $\epsilon$ 을 번역 전에 만들어내도록 강제한다. 논문의 모델은 문장 앞의 $\epsilon$ 도 고려함으로써 target 문장 사이에 삽입해야 할 $\epsilon$ 의 개수를 줄인다.  위의 과정 후에 source/target 문장의 길이가 동일하지 않을 경우 짧은 문장 뒤에 $\epsilon$ 을 추가해 두 문장의 길이를 맞춘다.Model  매 타임 스텝마다 모델은 source 언어의 현재 입력 단어와 target 언어의 출력 단어를 각각 $E$ 차원으로 임베딩한다.   이 두 벡터는 $2E$ 차원으로 concat 된 뒤, multi-layer LSTM에 입력된다. LSTM의 출력은 FC layer를 이용해 $E$ 차원으로 변환된다. 이 벡터는 출력 임베딩과 softmax 를 이용해 target 사전의 distribution으로 변환된다. 모델은 또한 source 언어의 입력 임베딩, target 언어의 입력 임베딩, 출력 임베딩을 공유한다.  논문에서 소개하는 모델은 Zaremba et al. (2014)의 recurrent language model과 매우 닮아 있다. 모델에서처럼, 논문에서도 teacher forcing과 cross-entropy loss를 사용한다. padding 심볼도 target sentence의 한 부분으로 다뤄졌고, 이를 위한 별도의 loss나 objective funtion은 사용되지 않았다.  또한 다른 번역 모델과 달리 논문의 모델은 inference 중에 일정 양의 메모리만 사용한다. 메모리에 직전 히든 스테이트만 저장하고 인코딩된 이전 단어들의 representation은 저장하지 않는다. 이를 통해 decoding complexity는 최악의 경우 $\mathscr{O}(n+m)$ 이다.Aligned Batchingpreprocessing 이후에 모든 source-target 쌍은 동일한 길이로 변한다. 논문에서는 이 다음 source 문장과 target 문장을 각각 순서를 유지한 string으로 합친다. 이를 통해 모델은 마치 language model을 학습하는 것처럼 학습할 수 있다. 구체적으로는 BPTT 하이퍼파라미터가 지정된다. 각 배치의 모든 요소는 BPTT source 토큰과 각각에 대응하는 target 토큰을 포함한다. language model을 학습할 때처럼, ($i-1$) 번째 배치의 마지막 히든 스테이트가 $i$ 번재 배치의 첫 히든 스테이트가 된다.Decoding논문에서는 예측 (inference) 중에 출력의 질을 향상시키기 위해 beam search를 조정해 사용한다. 구체적인 조정은 다음과 같다.  Padding limit: 논문에서는 $\epsilon$ 의 개수가 제한 개수에 다다르면 $\epsilon$ 의 등장 확률을 0으로 강제함으로써 패딩 토큰의 최대 개수에 제한을 두었다. 처음에 삽입되는 패딩 토큰은 이 제한 개수에 포함되지 않는다.  Source padding injection (SPI): 논문에서는 decoder가 source 언어의 EOS 토큰을 읽으면 이 EOS 토큰에 높은 확률을 부여한다는 것을 발견했다. 이에 논문에서는 SPI 파라미터를 $c$ 로 두었고, beam search에서는 0부터 $c$ 까지는 EOS 전까지를 $\epsilon$ 으로 간주한다.ExperimentsSetup  실험은 EN ⟷ FR, EN ⟷ DE 로 이루어졌고, 학습/검증/테스트 데이터는 각각 WMT 2014, newstest2013, newstest2014를 이용했다. 각 문장은 토크나이징과 30,000 BPE opeartion을 거친다.  네트워크의 구조는 먼저 4개의 LSTM 레이어 (1,000 units, embedding 500dim)를 이용한다. 학습 중 레이어와 임베딩에는 droutput이 적용된다. 배치 사이즈는 200, unroll step은 60이다. SGD를 사용했고, 첫 learning rate는 20이다. 논문에서는 6,500 스텝마다 검증 데이터에 대한 perplexity를 확인하고, 개선이 없으면 learning rate를 1/2로 줄인다.Results      eager 모델은 EN ⟷ FR에 대해서는 레퍼런스 모델보다 최대 0.8% 낮은 BLEU를 보였다. 그러나 EN ⟷ DE 에서는 최대 4.8% 레퍼런스보다 낮은 성능을 보였다.            문장 길이로 모델의 성능을 확인했을 때는 긴 문장에서는 eager 모델이 더 좋은 성능을 보였으나 짧은 문장에 대해서는 레퍼런스 모델보다 낮은 성능을 보였다. 이는 attention 기반의 방법이 짧은 문장 학습에 어려움을 보이기 떄문이다.  Conclusion  논문에서 소개한 방법은 토크나이징 결과와, source/target 문장 간의 어순, 닮음새에 영향을 많이 받을 것으로 보인다. 특히 한국어/영어 같이 문장의 서술 구조의 차이가 큰 경우에는 align이 쉽지 않을 뿐더러, 높은 성능을 기대하기도 어려울 것 같다.  다만 기존 NMT의 Encoder-Decoder 구조를 이용하지 않더라도 기존과 비슷한 성능을 보인다는 것은 모델의 구조 뿐만 아니라, 데이터 전처리 방법이 얼마나 중요한지 보여주는 대목이라고 생각된다.]]></content>
      <categories>
        
          <category> NLP </category>
        
          <category> Deep Learning </category>
        
      </categories>
      <tags>
        
          <tag> nlp </tag>
        
          <tag> deep learning </tag>
        
          <tag> review </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[[Review] Universal Transformers]]></title>
      <url>/nlp/deep%20learning/2018/09/27/Review-Universal_Transformers/</url>
      <content type="text"><![CDATA[  Mostafa Dehghani et al., Universal Transformers., 2018  해당 코드: https://github.com/tensorflow/tensor2tensorIntroduction  Transformer 모델 같은 convolutional &amp; fully-attentional feed-forward 아키텍처는 최근, MT와 같은 시퀀스 모델링에서 RNN의 대체제로 떠올랐다. Transformer 모델은 self-attention 메커니즘을 통해 입출력 심볼의 context-informed vector-space representation을 학습한다. 이 representation은 모델이 symbol-by-symol로 출력 시퀀스를 예측하기 때문에, 서브시퀀트 심볼의 분산을 예측한다. Transformer 모델의 이러한 점은 병렬화하기 쉽고, 각 심볼의 representaion이 다른 심볼의 representaion에 의해 직접적인 정보를 담기 때문에, 효과적인 receptive field를 얻을 수 있다는 이점을 가져온다.  Transformer 모델은 반복적이고 재귀적인 transformation을 학습하는 RNN의 inductive bias에 앞서지만, 이 inductive bias가 언어의 복잡성을 학습하는데 중요한 역할을 한다는 것을 실험 결과가 보여준다. 이 때문에 Transformer는 계산적으로 일반화하기 매우 어렵다. Universal Transformer는 이러한 단점들을 극복한다.ModelThe Universal Transformer  Universal Transformer는 sequence-to-sequence 모델에서 흔히 쓰이는 encoder-decoder 구조에 기반한다.  그러나 Universal Transformer는 시퀀스 위치를 순환하지 않고, 각 위치의 vector representaion의 연속적인 갱신(revision)을 순환한다는 점에서 기존의 RNN과 가장 큰 차이를 가진다. 즉 Universal Transformer는 시퀀스 내의 심볼 개수에 구애받지 않고, 각 심볼의 representaion의 업데이트 횟수에 귀속된다.  매 스텝마다 각 위치에서의 representation은 2단계에 걸쳐 갱신된다.          먼저, Universal Transformer는 self-attention 메커니즘을 사용해 모든 위치에서 정보를 교환하고 각 위치의 representation을 생성한다. 이 representation은 이전 타임 스텝의 representation에 영향을 받는다.      그 다음, 각 위치에서 독립적으로 self-attention 출력값에 shared transition을 적용한다. 이 점이 레이어를 쌓는 Transformer나 RNN 등 유명한 neural 시퀀스 모델과 가장 차별되는 점이다.                Encoder로는 $m$ 길이의 입력이 주어졌을 때, $d$ 차원의 임베딩으로 초기화된 행렬을 이용한다. ($H^0 \in \mathbb{R}^{m \times d}$) Universal Transformer는 그 다음 반복해서 $t$ 스텝에서의 $m$ 위치의 represantation $H^t$을 계산하는데, 이 때 multiheaded dot-product self-attention, recurrent transition을 적용한다. residual connection과 dropout, layer normalization 또한 함께 적용된다.          작업에 따라 transition은 separable convolution이나 fully-connected NN (with relu) 중 하나가 사용된다.        $T$ 스텝 이후에 Universal Transformer의 최종 출력값은 입력 시퀀스의 $m$ 심볼의 $d$ 차원 representation 행렬이다. ($H^T \in \mathbb{R}^{m \times d}$)  Decoder는 기본적으로 encoder와 동일한 구조를 가진다. 하지만, decoder는 self-attention 이후에 decoder represention에서 얻은 쿼리 $Q$ 와 encoder representation을 projection해서 얻은 key/value $K, V$ 를 이용해 입력 시퀀스 각 위치의 최종 encoder representation $H^T$ 으로 향하는 attention을 추가로 계산한다.  학습 동안 Decoder  입력은 encoder-decoder 구조와 동일하게 오른쪽으로 하나의 위치만큼 이동한 목표 출력이다.  마지막으로 목표 심볼 distribution은 최종 decoder state에서 출력 사전 크기 $V$ 로 affine 변환 $O \in \mathbb{R}^{d \times V}$ 과 softmax를 통해 얻어진다.The Adaptive Universal Transformer  시퀀스 프로세싱 중에, 특정 심볼들은 다른 것들보다 모호할 때가 있어 이 심볼들을 처리하는 데 자원을 더 쏟는 것이 필요하다. 이 때 각 심볼에 필요한 계산량을 조절하는 ACT (Adaptive Computation Time)을 Universal Transformer에 적용한 것을  The Adaptive Universal Transformer라고 부른다.Experiments  논문에서는 총 6가지 태스크를 통해 Universal Transformer를 평가했다. 6가지 태스크는 아래와 같다.          bAbI Question-Answering      Subject-Verb Agreement      LAMBADA Language Modeling      Algorithmic Tasks      Learning to Execute (LTE)      Machine Translation      bAbI Question-Answering  bAbI Question-Answering 데이터셋은 주어진 영어 문장에서 supporting facts를 인코딩하는 질문에 답하는 20가지의 태스크로 이루어져 있다.  Adaptive Universal Transformer가 10K/1K 모두에서 SOTA 성능을 보였다.Subject-Verb Agreement  subject와 verb 일치를 평가하는 태스크로, hierarchical (dependency) 구조를 얼마나 잘 잡아내는 지 확인하는 태스크이다.  Universal Transformer는 기존의 Transformer보다 나은 성능을 보였고, Adaptive Universal Transformer는 SOTA와 견줄만한 성능을 보였다.LAMBADA Language Modeling  LAMBADA Language Modeling 태스크는 주어진 문장을 통해 공백 단어를 예측하는 문제이다.  해당 태스크에서도 Universal Transformer가 SOTA 성능을 이뤄냈다.Algorithmic Tasks  Universal Transformer가 LSTM과 Transformer보다 나은 성능을 보였다.  Neural GPU가 완벽한 결과를 보이지만, 이는 특별한 과정이 추가되었기 때문이며, 다른 모델은 그렇지 않다.Learning to Execute (LTE)  이 태스크는 컴퓨터 프로그램을 실행하도록 학습할 수 있는지 평가하는 문제이다.  Universal Transformer는 모든 태스크에서 완벽한 결과를 보였다.          위: char-acc (maximum length of 55)      아래: char-acc (maximum nesting of 2 nad length of 5)      Machine Translation  MT 태스크는 WMT 2014 영어-독일어로 평가되었다.  Universal Transformer (with fully-connected recurrent function without ACT) 가 기본 Transformer보다 BLEU를 0.9 향상시켰다.Universality and Relationship to Other Models  Universal Transformer는 end-to-end Memory Network와도 관련되어 있다. 그러나 end-to-end Memory Network와는 다르게 Universal Transformer는 개별 입/출력 위치에 정렬된 스테이트에 해당하는 메모리를 사용한다. 또한, Universal Transformer는 encoder-decoder 구조를 따르며 대규모 sequence-to-sequence 태스크에서 좋은 성능을 보인다.Conclusion  Universal Transformer는 다음의 주요 속성을 하나의 모델에 결합했다.          Weight sharing                  CNN/RNN에서 사용되는 weight sharing을 도입해 소규모/대규모 태스크에서 inductive bias와 모델의 표현력 사이에서 경쟁력 있는 능력을 갖췄다.                    Conditional Computation                  Universal Transformer는 ACT를 도입해 fixed-depth Universal Transformer보다 더 강력한 능력을 갖췄다. (The Adaptive Universal Transformer)                    AppendixDetailed Schema of the Universal Transformer]]></content>
      <categories>
        
          <category> NLP </category>
        
          <category> Deep Learning </category>
        
      </categories>
      <tags>
        
          <tag> nlp </tag>
        
          <tag> deep learning </tag>
        
          <tag> review </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[[Review] Using millions of emoji occurrences to learn any-domain representations for detecting sentiment, emotion and sarcasm]]></title>
      <url>/deeplearning/nlp/2018/08/26/Review-Using-millions-of-emoji-occurrences-to-learn-any-domain-representations-for-detecting-sentiment,-emotion-and-sarcasm.md/</url>
      <content type="text"><![CDATA[  Bjarke Felbo, Alan Mislove, Anders Søgaard, Iyad Rahwan, Sune Lehmann. 2017. Using millions of emoji occurrences to learn any-domain representations for detecting sentiment, emotion and sarcasm. ACL 2017, pages 1615–1625Introduction  레이블링 된 데이터의 부족으로 NLP 작업이 제한되는 경우가 있다. 이런 이유로 문장에 나타나는 감정 표현은 소셜 미디어 감성 분석과 연관 작업에서 representation을 학습하는 데에 유용하게 사용되고 있다.  이를 이용한 State-of-the-art 접근법으로는 소셜 미디어 감성 분석에 긍정/부정 이모티콘을 사용했고 (Deriu et al., 2016; Tang et al., 2014), 이와 비슷하게 #anger, #joy 같은 해시태그를 사용해 감정 분석에 접근한 방법도 있다. (Mohammad, 2012)  노이즈가 섞인 레이블을 이용한 Distant supervision은 많은 경우에 모델의 성능을 향상시킨다. 본 논문에서는 더 다양한 노이즈가 섞인 레이블로 Distant supervision을 확장하고, 이러한 시도가 텍스트에서 더 풍부한 감정 representation 학습을 가능케 했다. (DeepMoji) 본 논문에서는 단일 pretrained model을 5가지 도메인으로 일반화한다.MethodPretraining  논문은 많은 경우에 이모지가 텍스트의 감정 정보를 시사하는 사실을 바탕으로, 이모지를 예측하는 분류 모델을 미리 학습하는 것이 목표 작업의 성능을 향상시키다고 말한다.  데이터로는 2013.01.01~ 2017.06.01 간의 트윗을 사용했으나, 이모지가 포함된 텍스트라면 상관 없다. 한편, URL이 포함된 트윗은 해당 URL이 감정 표현의 대상이 된다고 가정하고 URL이 없는 트윗만 사용되었다.  토크나이징은 단어 기준으로 진행되었고, 중복된 토큰은 정규화되었다. (‘loool’ = ‘looooool’) URL (벤치마크 데이터셋)과 숫자도 마찬가지로 정규화되었다.  이모지에 대해서는, 사용된 이모지의 개수와 상관없이 같은 종류의 이모지면 pretraining을 위한 데이터로 사용되었다.Model  DeepMoji 모델 구성은 다음과 같다.          Embedding (256 dim) &amp; tanh (L2 regularization of 1E-6)      2 bi-LSTM (1024 dim, 512 dim each)      Attention layer      Transfer Learning  pretrain 된 모델은 transfer learning을 통해 목적 작업에 적용될 수 있는데, 본 논문에서는 “chain-thaw”라는 간단한 접근법을 소개한다. “chain-thaw” 방법은 순차적으로 가중치를 고정하고, 한번에 하나의 레이어만 업데이트하는 방법이다.  구체적으로는 먼저 어느 한 레이어를 학습시키고, (보통 Softmax 레이어) 그 다음 첫 번째 레이어부터 순차적으로 업데이트한다. 마지막에는 모든 레이어가 업데이트된다.  chain-thaw을 통해 오버피팅의 리스크를 줄이면서 어휘를 새로운 도메인으로 확장할 수 있다.ExperimentsEmoji prediction  pretraining 데이터에서 트윗은 하나의 이모지에 대응하도록 복사되었고, 이렇게 준비된 데이터는 트윗 16억 개이다. validation/test 데이터로는 각각 640K 개 (각 이모지 당 10K 개)가 사용되었다. 나머지 트윗은 모두 upsampling을 거쳐 학습 데이터로 사용되었다.  DeepMoji 모델은 pretraining task로 평가되었고, 결과는 아래의 표와 같다. top 1과 top 5 정확도 모두 노이즈가 섞인 이모지 레이블로 평가되었다. (어떤 문장에도 적합한 이모지 등)                   Params      Top 1      Top 5                  Random             1.6%      7.8%              fasttext (d = 256)      12.8      12.8%      36.2%              DeepMoji (d = 512)      15.5      16.7%      43.3%              DeepMoji (d = 1024)      22.4      17.0%      43.8%        fastText만 사용한 모델은 DeepMoji의 임베딩 레이어만 사용한 것과 동일한 결과를 냈다. 한편, fastText와 DeepMoji의 Top 5 정확도 차이는 이모지 예측의 어려움을 보여준다.  DeepMoji는 임베딩과 Softmax 레이어 사이에 어텐션 레이어가 있는 LSTM 레이어를 가지는데, 이 차이가 각 단어의 context를 잡아내는 데 중요한 역할을 했다고 말해준다.  요즘의 모델들은 기본적으로 biLSTM과 어텐션 레이어 등을 포함하는데, (biLSTM은 거의 기본값이 된 듯 한다.) 이것은 그만큼 biLSTM과 어텐션의 성능이 NLP 작업에서 뛰어나다는 것을 보여준다고 생각한다.Benchmarking  논문에서는 5개 도메인의 8개 데이터를 이용해 3개의 NLP 작업을 수행해 모델의 벤치마크 성능을 평가했다.  평가 metric으로는 감정 분석과 sarcasm에는 F1이, 감성 분류에는 정확도가 사용되었다.  벤치마킹 결과는 chain-thaw 방법을 활용한 DeepMoji 모델은 state-of-the-art 모델보다 모든 데이터셋에서 더 높은 성능을 보였다. 벤치마킹 결과는 DeepMoji의 좋은 성능에는 chain-thaw 영향이 크다는 것을 말해준다.Model Analysis  DeepMoji와 이전의 distant supervison 방법들의 가장 큰 차이는 DeepMoji는 노이즈가 섞인 레이블을 다양하게 활용했다는 점이다. 다양한 이모지의 영향을 알아보기 위해 1/8로 줄인 이모지 데이터셋 (긍/부정)을 활용한 결과, 벤치마크 상의 성능은 데이터의 크기보다 레이블의 다양성과 더 연관이 있었다.  이모지는 비슷한 감정을 나타내지만, 문맥마다 미묘한 차이를 나타낸다. DeepMoji는 이러한 미묘함을 학습했고, 이는 성능 향상으로 이어졌다는 것이다.  transfer learning 성능 향상에 대해 논문은 이렇게 추측한다. 1) skip connections를 적용한 어텐션 메커니즘 덕분에 모델이 어느 time step에서도 low-level features에 쉽게 접근해서 새로운 작업에 이용할 수 있다는 점, 2) 작은 데이터셋으로의 transfer learning에서 skip connections 덕분에 출력 레이어에서 초기 레이어로의 gradient 흐름이 개선된 점Conclusion  이 논문은 이모지를 이용해 감정 정보를 학습했고, 개선된 transfer learning으로 NLP 작업에서의 성능을 개선했다. DeepMoji 모델 또한 감정이라는 context를 학습한 contextualized embedding이라고도 볼 수 있을 것 같다.  또한 살펴볼 것은, 2-layer BiLSTM과 어텐션 메커니즘, skip connections의 사용이다. 최근 소개되는 state-of-the-art 모델들은 거의 대부분 어텐션 메커니즘이 추가된 BiLSTM을 이용하고 있다. 그만큼 NLP 작업에서 BiLSTM과 어텐션 메커니즘의 성능이 뛰어나다는 것일 것이다.  다만, 이러한 큰 네트워크를 학습하기 위해서는 그만큼 풍부한 데이터도 중요하다고 생각한다. State-of-the-art contextualized embedding인 ELMo의 경우에도 4096 차원의 아주 큰 BiLSTM과 1B token dataset을 이용한 BIG-CNN-LSTM (char-CNN, 2048 filteres)을 이용한다. 결국 context 학습이 최종 목적이 아니라, 이 모델을 다른 작업에 transfer learning으로 적용하자고 하는 것이기 때문에 더더욱 그럴 것이라고 생각한다. CV 분야에서도 미리 학습된 VGG 등을 이용해 transfer learning을 적용하는 것과 궤를 같이 한다고 생각한다.  그러나 여기서 한국어 NLP에 적용하기 위한 한계점도 있는데, 영어권 자료에 비해 대규모 한국어 데이터셋은 공개된 것이 많이 부족하다는 것이다. 세종 코퍼스는 2000년대 초반에 종료된 프로젝트이며, 그 양도 많지 않아 아쉬울 뿐이다.]]></content>
      <categories>
        
          <category> DeepLearning </category>
        
          <category> NLP </category>
        
      </categories>
      <tags>
        
          <tag> nlp </tag>
        
          <tag> deep learning </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[[프로세스와 스레드] 01. 프로세스와 스레드]]></title>
      <url>/python/2018/08/12/%ED%94%84%EB%A1%9C%EC%84%B8%EC%8A%A4%EC%99%80-%EC%8A%A4%EB%A0%88%EB%93%9C-01-%ED%94%84%EB%A1%9C%EC%84%B8%EC%8A%A4%EC%99%80-%EC%8A%A4%EB%A0%88%EB%93%9C/</url>
      <content type="text"><![CDATA[  출처: 컴퓨터 사이언스 부트캠프 with 파이썬 (양태환, 길벗)멀티프로세스와 멀티스레드를 자주 사용하게 되었는데, 프로세스/스레드의 개념과 파이썬의 멀티프로세싱/멀티스레딩의 차이를 알아봅니다. 이미지와 글의 출처는 참고로 명시한 책에서 가져왔습니다.1. 프로세스  프로그램: 하드디스크에 저장된 실행 파일. 실행하지 않는 이상 하드디스크에 계속 남아 있으며, 같은 경로에 같은 이름으로 동시에 존재할 수는 없다.  프로세스: 프로그램을 실행한 상태. 하드디스크에서 메인 메모리로 코드와 데이터를 가져와 현재 실행되고 있는 상태. 프로세스는 동시에 여러 개가 존재할 수 있음1.1 프로세스 상태프로세스를 실행하려면 독자적인 메모리 공간과 CPU가 필요하다. 메모리는 가상 메모리를 사용해서 해결한다. CPU는 한번에 하나의 프로세스에만 할당할 수 있다. 여러 프로세스가 완벽하게 ‘동시에’ 실행되는 건 불가능하다. 프로세스 상태는 상황에 따라 변한다.  생성 (Created): 프로그램을 더블클릭했을 때 프로세스가 생성되면서 실행 가능 상태가 된다. 바로 실행되는 것이 아니라, 우선 실행 가능 상태가 되어 실행 중인 프로세스와 우선순위를 비교한 다음 실행하거나 순서를 기다린다.  실행 가능 (Waiting): 실행 가능 상태의 프로세스는 언제든지 실행할 준비가 되어 있다. 운영체제는 인터럽트가 발생했을 때 실행 가능 상태의 프로세스 중 다음으로 CPU를 할당받아 실행될 프로세스에 CPU를 할당받아 실행될 프로세스에 결정한 후, 실행 중인 프로세스와 교체한다. 이 때 다음으로 실행될 프로세스에 CPU를 할당하는 것을 디스패치(dispatch)라고 하고, 실행 중이던 프로세스에서 CPU를 해제하는 것을 프리엠션(preemption)이라고 한다.  실행 (Running): 프로세스가 운영체제로부터 CPU를 할당받아 실행되고 있는 상태  보류 (Blocked): 프로세스가 I/O 작업을 하면 CPU를 해제하고 보류 상태로 변경된다. 이 때 실행 가능 상태의 프로세스 중 하나가 CPU를 할당받는다. I/O 작업이 완료된 다음 바로 실행 가능 상태로 변경되는 것이 아니라, 실행 가능 상태가 되어 실행되기를 기다린다. 또 Waiting는 언제든지 다시 실행될 수 있는 상태를 말하지만, 보류 상태는 I/O 작업이 끝나기 전에는 실행이 불가능한 상태이다.  소멸 (Terminated): 프로세스 실행이 완료되어 메인 메로리에서 사라진다.1.2 스케줄링스케줄링(scheduling)이란 운영체제가 여러 프로세스의 CPU 할당 순서를 결정하는 것이다. 이 일을 하는 프로그램을 스케줄러라고 한다.스케줄링은 CPU를 언제 할당하는지에 따라 선점형 스케줄링(preemptive scheduling)과 비선점형 스케줄링(non-preemptive scheduling)으로 나눌 수 있다.선점형 스케줄링에서는 어떤 프로세스가 실행 중에 있어도 스케줄러가 강제로 실행을 중지하고 다른 프로세스에 CPU를 할당할 수 있다. 비선점형 스케줄링에서는 실행 중인 프로세스가 종료되거나 I/O 작업에 들어가거나 명시적으로 CPU를 반환하기 전까지는 계속해서 실행된다. 우선순위가 높은 프로세스가 생성되어도 실행 중인 프로세스가 자발적으로 CPU를 양보하기 전까지는 실행될 수 없다.1.3 컨텍스트 스위칭프로세스 두 개가 같은 프로그램에서 만들어졌을 때 두 프로세스는 독립된 메모리 공간을 가진다. 프로세스가 실행되려면 다양한 CPU 레지스터 값과 프로세스 상태 정보 등이 필요하다. 그러므로 프로세스가 실행 상태에서 실행 가능 상태로 변경될 때 이러한 정보를 메모리 어딘가에 저장해야 한다. 프로세스의 CPU 상태와 프로세스의 상태를 저장해 둔 메모리 블록을 프로세스 제어 블록(Process Control Block, PCB)이라고 한다.스케줄러가 실행 중인 프로세스에서 CPU를 해제하고 실행 가능 상태의 프로세스에 CPU를 할당할 때, 실행 중인 프로세스의 CPU 상태 정보를 그 프로세스의 PCBdp 저장하고 곧 실행될 프로세스의 PCB에서 이전 CPU 상태 정보를 CPU로 가져오는 것을 컨텍스트 스위칭(context switching)이라고 한다. CPU 상태를 컨텍스트라고 부르는데 말 그대로 현재 CPU의 레지스터 값들을 전환하는 것이다.2. 스레드스레드(thread)란 프로세스 안의 실행 흐름의 단위로 스케줄러에 의해 CPU를 할당받을 수 있는 인스트럭션의 나열이다. 프로세스는 하나 이상의 스레드로 구성된다.프로세스가 PCB를 갖는 것처럼 스레드는 스레드 제어 블록(Thread Control Bock, TCB)을 갖는다. TCB에는 스레드 ID, 각종 레지스터 정보, 스레드 상태 정보, 스레드가 속해 있는 프로세스의 TCB 주소 등이 저장되어 있다.프로세스와 스레드 모두 인스트럭션의 나열이고 유사한 정보가 든 메모리 블록을 갖는다. 프로세스가 단일 스레드로 작동하면 프로세스와 스레드는 차이가 없다. 프로세스와 스레드의 차이점을 알려면 멀티프로세스와 멀티스레드를 비교해야 한다.2.1 멀티프로세스와 멀티스레드단일 코어 CPU에서 여러 개의 실행 흐름이 동시에 필요하다고 가정하면, 실행 흐름사이에서 데이터를 공유해야 한다. 실행 흐름은 결국 CPU를 점유하고 인스트럭션을 실행하는 것을 말하므로 여러 실행 흐름을 구현하려면 멀티프로세스나 멀티스레드로 구현해야 한다.먼저 멀티프로세스로 구현한다고 가정하면, 프로세스는 서로 독립적인 메모리 공간을 가지므로 기본적으로 데이터를 공유할 수 없다. 멀티프로세스에서는 모든 프로세스가 서로 다른 메로리 공간을 가지므로 데이터를 공유하려면 특별한 기법을 사용해야 한다.하지만 멀티스레드로 구현하면 데이터를 쉽게 공유할 수 있다. 멀티프로세스와 달리 여러 스레드가 스택만 서로 다른 공간을 갖고, 코드, 데이터, 힙은 공유하기 때문이다.스레드는 각자 독립적인 스택 세그먼트를 갖지만, 코드, 데이터, 힙은 다른 스레드와 공유한다. 데이터 세그멘트나 힙 세그먼트에 공유 데이터를 두면 모든 스레드가 이용할 수 있다.li = [i for i in range(1000+1)]for idx in range(1000+1):    li[idx] *= 2아래 예는 위의 싱글 스레드 작업을 멀티 스레딩으로 바꾼 코드이다.import threadingdef thread_main(li, i):    for i in range(offset*i, offset*(i+1)):        li[i] *= 2num_elem = 1000num_thread = 4offset = num_elem // num_threadli = [i+1 for i in range(num_elem)]threads = []for i in range(num_thread):    th = threading.Thread(target=thread_main,                           args=(li, i))    threads.append(th)    for th in threads:    th.start()    for th in threads:    th.join()print(li[:100])&gt;&gt;&gt;[2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32, 34, 36, 38, 40, 42, 44, 46, 48, 50, 52, 54, 56, 58, 60, 62, 64, 66, 68, 70, 72, 74, 76, 78, 80, 82, 84, 86, 88, 90, 92, 94, 96, 98, 100, 102, 104, 106, 108, 110, 112, 114, 116, 118, 120, 122, 124, 126, 128, 130, 132, 134, 136, 138, 140, 142, 144, 146, 148, 150, 152, 154, 156, 158, 160, 162, 164, 166, 168, 170, 172, 174, 176, 178, 180, 182, 184, 186, 188, 190, 192, 194, 196, 198, 200]2.3 경쟁 조건import threadingg_count = 0def thread_main():    global g_count    for i in range(100000):        g_count += 1        threads = []for i in range(50):    th = threading.Thread(target=thread_main)    threads.append(th)    for th in threads:    th.start()    for th in threads:    th.join()    print('g_count: {:,}'.format(g_count))&gt;&gt;&gt;g_count: 4,902,146전역 변수 g_count를 선언하고 thread_main() 함수 안에서 g_count 값에 1을 100,000번 더한다.스레드를 총 50개 만들었으니 스레드 50개가 동시에 g_count에 접근해 값을 수정하려고 시도한다. 이처럼 여러 스레드가 동시에 접근, 수정, 공유 가능한 자원을 공유 자원이라고 한다. 위 코드에서는 g_count라는 전역 변수가 공유 자원이다.g_count의 최종 값은 스레드가 각각 100,000번 씩 값을 증가시키므로 5,000,000이 될 것 같지만, 실행 결과는 매번 달라진다.이상적인 경우에서는 먼저 전역 변수 g_count 값을 범용 레지스터로 가져와 값을 증가시키고, 연산이 끝난 레지스터 값을 g_count에 저장한다. 이제 컨텍스트 스위칭이 일어나 스레드 2에 CPU가 할당된다. 이 과정이 반복되는 것이 이상적인 경우이지만 선점형 스케줄링에서는 스레드 1의 연산이 완전히 끝날 때까지 컨텍스트 스위칭을 기다려 주지 않는다.g_count에 값을 저장하기 전에 컨텍스트 스위칭이 일어나면 다른 스레드가 이전 레지스터 값을 복원하고 이전 상태에 이어 연산을 마무리한다. 각 스레드의 관점에서 보면 스레드는 첫 번째 경우나 두 번째 경우 모두 g_count 값에 1을 더하는 같은 연산을 하지만 컨텍스트 스위칭이 언제 일어나는지에 따라 전혀 다른 결과가 나온다. 이처럼 스레드 여러 개가 공유 자원엗 동시에 접근하는 것을 경쟁 조건(race condition)이라고 한다.만약 스레드 안에 있는 코드가 공유 자원에 접근해 변경을 시도하는 코드(임계 영역, critical section)가 있으면 문제가 발생한다.2.4 상호 배제경쟁 조건 문제를 해결하기 위해서 상호 배제(mutual exclusion)을 사용한다. 상호 배제의 원리는 간단하며, 스레드 하나가 공유 자원을 이용하는 동안에는 다른 스레드가 접근하지 못하게 막는 것이다. 파이썬에서는 주로 Lock 객체를 활용한다.import threadingg_count = 0def thread_main():    global g_count    # 한 스레드가 lock을 획득하면    # 획득을 시도한 나머지 스레드는 대기한다.    lock.acquire()    for i in range(100000):                g_count += 1    # lock 반환    # 대기하던 스레드 중 하나가 획득    lock.release()        lock = threading.Lock()        threads = []for i in range(50):    th = threading.Thread(target=thread_main)    threads.append(th)    for th in threads:    th.start()    for th in threads:    th.join()    print('g_count: {:,}'.format(g_count))&gt;&gt;&gt;g_count: 5,000,000이제 원하는 대로 결과가 나온다.]]></content>
      <categories>
        
          <category> python </category>
        
      </categories>
      <tags>
        
          <tag> python </tag>
        
          <tag> computer science </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[SVM - 이론을 중심으로]]></title>
      <url>/machinelearning/2018/07/08/SVM-%EC%9D%B4%EB%A1%A0%EC%9D%84-%EC%A4%91%EC%8B%AC%EC%9C%BC%EB%A1%9C/</url>
      <content type="text"><![CDATA[서포트 벡터와 마진선형 SVM 분류 모델은 판별 함수 $w^T \cdot x - w_0 = w_1x_1 + \cdots w_nx_n - w_0$ 를 계산해서 새로운 샘플 $x$ 의 클래스를 예측한다. $y$ 데이터는 $+1, -1$ 두 개의 값을 가지고, 이를 분류하는 문제를 풀어야 한다. 판별 함수의 정의에 따라 $y$ 값이 $+1$ 인 데이터 $x_+$ 에 대한 판별 함수 값은 양수가 되며, 반대로 $y$ 값이 $-1$ 인 데이터 $x_-$ 에 대한 판별 함수 값은 음수가 된다.$y$ 값이 $+1$ 인 데이터 중에서 판별 함수의 값이 가장 작은 데이터를 $x^+$ 라 하고 $y$ 값이 $-1$ 인 데이터 중에서 판별 함수의 값이 가장 큰 데이터를 $x^-$ 라고 하면, 이 데이터들은 각각의 클래스에 속한 데이터 중에서 가장 경계선에 가까이 있는 데이터이다. 이 데이터를 서포트 벡터 (support vector) 라고 한다.부호만 지키면 되므로 실제 $f(x^+)$ 와 $f(x^-)$ 값은 어떤 값이 되어도 상관없으므로 다음과 같이 가정한다.판별 경계선과 데이터 $x^+$, $x^-$ 사이의 거리는 다음과 같다.이 거리의 합을 마진 (margin) 이라고 하며 마진이 클 수로 경계선이 더 안정적이라고 할 수 있다.마진이 최대가 되는 경우는 $| w |$, 즉 $| w |^2$ 가 최솟값인 경우와 같다. 즉, 다음과 같은 목적 함수를 최소화한다.  $| w |$ 를 최소화하는 대신 $\dfrac{1}{2} | w |^2$ 인 $\dfrac{1}{2} w^T w$ 를 최소화한다. 이는 $\dfrac{1}{2} w^T$ 가 깔끔하고 간단하게 미분되기 때문이다. 반면 $| w |$ 는 $w = 0$ 에서 미분할 수 없다.또한 모든 표본 데이터를 제대로 분류해야 하므로 모든 데이터에 대해 다음 조건을 만족해야 한다.라그랑주 승수법을 사용하면 목적 함수를 다음과 같이 나타낼 수 있다.Dual Problem (쌍대 문제, Dual Form)원 문제 (primal problem) 라는 제약이 있는 최적화 문제가 주어지면 쌍대 문제 (dual problem) 라는 다른 문제로 표현할 수 있다. 일반적으로 쌍대 문제 해는 원 문제 해의 하한값이지만, 어떤 조건 하에서는 원 문제와 동일한 해를 제공한다. SVM은 이 조건을 만족시킨다.최적화 조건은 목적 함수 $L$ 을 $w$, $w_0$ 으로 미분한 값이 0이 되어야 하는 것이다.이 식을 풀어서 정리하면 다음과 같아진다.즉 아래와 같다.이 두 수식을 원래의 목적 함수에 대입하여 $w$, $w_0$ 을 없애면 다음과 같다.정리하면 다음과 같다.이 때 $a$ 는 다음 조건을 만족한다.이 식을 최소화하는 $a$ 를 찾으면 예측 모형을 다음과 같이 나타낼 수 있다.]]></content>
      <categories>
        
          <category> MachineLearning </category>
        
      </categories>
      <tags>
        
          <tag> machine learning </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[KKT (Karush-Kuhn-Tucker) 조건]]></title>
      <url>/machinelearning/2018/07/08/KKT-Karush-Kuhn-Tucker-%EC%A1%B0%EA%B1%B4/</url>
      <content type="text"><![CDATA[라그랑주 승수법 (Lagrange Multiplier)등식 제한 조건이 있는 최적화 문제는 라그랑주 승수법을 사용해 최적화할 수 있다. 라그랑주 승수법에서는 $f(x)$ 가 아닌라는 함수를 목적함수로 보고 최적화한다. $h$ 는 독립 변수 $\lambda$ 가 추가되었으므로 다음 조건을 만족해야 한다.위의 $N+M$ 개의 연립방정식을 풀면 $N+M$ 개의 미지수 $x_1, x_2, \ldots, x_N, , \lambda_1, \ldots , \lambda_M$ 를 구할 수 있는데, 여기에서 $x_1, x_2, \cdots, x_N$ 이 제한 조건을 만족하는 최솟값 위치를 나타낸다.KKT 조건$g(x) \le 0$ 이라는 부등식 제한 조건이 있는 최적화 문제에서도 마찬가지로 라그랑주 승수법과 마찬가지로를 목적함수로 보고 최적화한다.단, 이 경우 최적화 필요 조건은 등식 제한 조건의 경우와는 달리 KKT(Karush-Kuhn-Tucker) 조건으로 부르며, 다음의 3개의 조건으로 이루어진다.  모든 독립 변수에 대한 미분이 0: $\dfrac{\partial h(x, \lambda)}{\partial x_i} = 0$  모든 라그랑주 승수와 부등식의 곱이 0: $\lambda_j \cdot \dfrac{\partial h(x, \lambda)}{\partial \lambda_j} = \lambda \cdot g_j = 0$  음수가 아닌 라그랑주 승수: $\lambda_j \ge 0$]]></content>
      <categories>
        
          <category> MachineLearning </category>
        
      </categories>
      <tags>
        
          <tag> python </tag>
        
          <tag> machine learning </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[Loss Function and Gradient]]></title>
      <url>/machinelearning/2018/07/04/Loss-Function-and-Gradient/</url>
      <content type="text"><![CDATA[  그 동안 loss function과 gradient를 공부한 뒤 수학적 의미를 자세히 복습하지 않아서 다시 해당 내용을 복습!최적화 문제와 Loss Function최적화 문제는 모수를 입력으로, 예측 오차를 출력으로 하는 함수 $f$ 의 값을 최소화하는 $x$ 의 값 $x^{\ast}$ 를 찾는 것이다.이 때 최소화 하려는 함수를 Objective/Cost/Loss Function 등으로 부른다.수치적 최적화 (Numerical Optimation)반복적 시행 착오로 최적화 필요조건을 만족하는 $x^{\ast}$ 를 찾는 방법을 수치적 최적화라고 한다. 수치적 최적화는 함수 위치가 최적점이 될 때까지 가능한한 적은 횟수만큼 $x$ 의 위치를 옮기는 방법이다. 이는 두 가지로 구성된다.  현재 위치 $x_k$ 가 최적점인지 판단하는 알고리즘  어떤 위치를 시도한 뒤, 다음 번에 시도할 위치 $x_{k+1}$ 을 찾는 알고리즘기울기 필요 조건현재 시도하고 있는 위치가 최소점인지 알아내기 위해 미분을 이용한다. $x^{\ast}$ 가 최소점이 되기 위해서는 $x^{\ast}$ 에서의 함수의 기울기 $\dfrac{df}{dx}$ 의 값이 0이어야 한다. 이를 기울기 필요 조건이라 한다.즉,을 만족해야 한다.이 때 함수 $f$ 의 편미분 값을 그래디언트(Gradient) 라고 한다.SGD (Steepest Gradient Descent)SGD 방법은 현재 위치에서의 기울기 $g(x_k)$ 만을 이용해 다음에 시도할 위치를 알아내는 방법이다.현재 위치 $x_k$ 에서 기울기가 음수이면 앞으로 진행하고, 양수이면 뒤로 진행해 점점 낮은 위치로 이동한다. $g(x_k) = 0$ 이면 최소점에 도달한 것이므로 더 이상 위치를 옮기지 않는다.간단히 정리하면 SGD는 현재 위치에서 (initial point) 가장 함수값이 작아지는 방향을 찾아서 (gradient) 이동하는 (next position) 알고리즘이다.]]></content>
      <categories>
        
          <category> MachineLearning </category>
        
      </categories>
      <tags>
        
          <tag> python </tag>
        
          <tag> machine learning </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[[Review] Searching for Activation Functions]]></title>
      <url>/deeplearning/2018/07/02/Review-Searching-for-Activation-Functions/</url>
      <content type="text"><![CDATA[  출처: Prajit Ramachandran, Barret Zoph, Quoc V. Le, Searching for Activation Functions, 2017Introductin  ReLU는 그 간편함에 시그모이드나 tanh보다 더 널리, 딥러닝 커뮤니티에서 각광받는 활성화 함수가 되었음  Maas et al., 2013; He et al., 2015; Clevert et al., 2015; Klambauer et al., 2017 등이 ReLU를 대체할 활성화 함수를 제안했지만 ReLU만큼 널리 채택되진 않았음. 많은 이들이 ReLU의 간편함과 안정성을 선호했는데 이는 다른 활성화 함수들은 다른 모델과 데이터셋에서 불안정한 모습을 보였기 때문임  ReLU를 대체할 활성화 함수들은 사람의 손으로 설계되었음. 그러나 Zoph &amp; Le, 2016; Bello et al., 2017; Zoph et al., 2017 등의 연구는 사람이 설계하던 부분을 자동화하는 검색 기술이 매우 효과적임을 보임. 특히 Zoph et al., 2017는 convolutional cell을 찾기 위해 강화학습 기반의 검색 기술을 사용했는데 ImageNet에서 사람이 설계한 아키텍처보다 더 높은 성능을 보임  본 논문에서는 새로운 활성화 함수를 찾기 위해 자동화 검색 기술을 사용했음. 특히 아키텍처를 변경하지 않으면서도 ReLU를 대체할 수 있는 스칼라 함수를 찾는데 중점을 두었음. (입력과 출력이 모두 스칼라) 철저한 검색과 강화학습 기반의 검색으로  유망한 여러 새로운 활성화 함수를 찾아냄.  실증적 평가를 거쳐 찾아낸 함수는 Swish로, 기존 시그모이드에 x 대신 beta・x를 입력으로 주며, 그 시그모이드값에 x를 곱한 형태이다. 베타는 상수 혹은 훈련 가능한 파라미터이다. ReLU를 Swish로 교체한 결과 ImageNet top-1 분류에서 Mobile NASNet-A은 0.9%, Inception-ResNet-v2은 0.6% 만큼 성능이 향상되었음. 이는 꽤 대단한데 Inception V3 (2016)에서 Inception-ResNet-v2 (2017)로 1.3% 성능 향상을 위해 일 년간 파라미터 튜닝을 거쳤기 때문Methods      검색 공간을 설계하는 데 있어 그 크기와 표현성의 밸런스가 필요        활성화 함수는 여러 core unit(이항 함수)의 반복으로 구성되었음            큰 검색 공간에는 RNN 컨트롤러를 사용함 (Zoph &amp; Le, 2016). 각 타임 스텝마다 컨트롤러는 활성화 함수의 한 요소를 예측.        RNN 컨트롤러는 검증 정확도를 최대화하기 위해 강화학습으로 훈련됨. 이는 RNN 컨트롤러가 가장 높은 검증 정확도를 내는 활성화 함수를 만들어내도록 유도  Search Findings  찾아진 활성화 함수들은 다음과 같음. 모든 함수는 ResNet-20을 child network로 사용하고 CIFAR-10 데이터에 대해 10K 스텝으로 실험되었음  좋은 성능을 보인 활성화 함수들은 다음과 같음          복잡한 함수는 간단한 함수보다 성능이 나쁨      좋은 성능을 내는 함수들은 이항 함수에 $x$ 를 그대로 넣는 $b(x, g(x))$ 의 형태. ReLU의 $g(x)$ 는 $g(x)=0$      나눗셈을 사용하는 함수들은 대체로 나쁜 성능을 보이는데 분모가 0에 가까우면 출력값이 폭주하기 때문. 나눗셈이 잘 동작할 때는 1) 분모가 0에서 멀거나, 분자 또한 0에 가까워 출력이 1이 될 경우        더 견고한 함수를 찾아내기 위해 텐서플로에서 RestNet-164 (RN), Wide ResNet 28-10 (WRN), DenseNet 100-12 (DN) 세 모델을 대상으로 ReLU를 해당 함수로 대체하는 실험을 진행함  모델의 변경에도 불구하고 8개 중 6개 모델이 성공적인 일반화 성능을 보임. $x \cdot \sigma (\beta x)$와 $\max(x, \sigma(x))$가 세 모델에서 모두 ReLU보다 높은 성능을 보임  더 좋은 일반화 성능을 보인 $x \cdot \sigma (\beta x)$, Swish 함수에 대해 ReLU와 비교하며 추가적인 평가를 진행함.Swish      Swish는 $\beta$ 가 0이면 스케일된 선형 ($x/2$) 이고 무한대로 갈수록 0-1 함수, 즉 ReLU에 가까워짐. $\beta$ 를 조절함으로써 선형과 ReLU 사이의 비선형 정도를 조절할 수 있음        ReLU와 마찬가지로 Swish또한 상하한선이 있음. 반면 ReLU와는 달리 좀 더 부드러우며, 단조롭지 않음. 이 점이 Swish가 다른 활성화 함수와 구별되는 점        ReLU와 Swish의 가장 큰 차이점인 입력값 $x$ 가 음수일 경우의 “bump”임.        더군다나 대부분의 딥러닝 라이브러리에서 Swish는 코드 한줄로 바로 실행할 수 있음 (x * tf.sigmoid(beta * x)   혹은 tf.nn.swish(x)). 또한 ReLU 사용시보다 learning rate를 약간 낮추는 편이 잘 동작했음  Experiments with Swish  ResNet-v2나 Transformer 등의 모델로 Swish와 여러 활성화 함수를 비교함Conclusion  Swish는 간단하며 ReLU와 비슷한데, 이는 네트워크에서 ReLU를 교체하는 것은 코드 한 줄이면 된다는 것을 의미]]></content>
      <categories>
        
          <category> DeepLearning </category>
        
      </categories>
      <tags>
        
          <tag> deep learning </tag>
        
          <tag> review </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[Batch Normalization]]></title>
      <url>/deeplearning/2018/05/29/Batch-Normalization/</url>
      <content type="text"><![CDATA[  참고: 밑바닥부터 시작하는 딥러닝 (사이토 고키 저, 개앞맵시 역, 한빛미디어)배치 정규화는 2015년 Sergey Ioffe, Christian Szegedy가 제안한 방법이다. 배치 정규화는 널리 사용되는 방법인데 그 이유는 다음과 같다.  학습을 빨리 진행할 수 있다.  초깃값에 크게 의존하지 않는다.  과대적합을 억제한다.배치 정규화의 기본 아이디어는 각 층에서의 활성화값이 적당히 분포되도록 조정하는 것이다. 학습 시 미니배치 단위로 정규화하는데 구체적으로는 데이터 분포가 평균이 0, 분산이 1이 되도록 정규화한다.여기에는 미니배치 $B = { x_1, x_2, \cdots, x_m }$ 이라는 $m$ 개의 입력 데이터의 집합에 대해 평균 $\mu_B$ 와 분산 $\sigma_B^2$ 를 구한다. 그리고 입력 데이터를 평균이 0, 분산이 1인 데이터 ${ \hat{x}_1, \hat{x}_2, \cdots, \hat{x}_m }$ 가 되도록 정규화한다.또 배치 정규화 계층마다 이 정규화된 데이터에 고유한 확대(scale)와 이동(shift) 변환을 수행한다.위 식에서 $\gamma$ 가 확대를, $\beta$ 가 이동을 담당한다. 두 값은 $\gamma = 1$, $\beta = 0$ 부터 시작한다. (1배 확대 및 0 이동 즉, 원본 그대로에서 시작)]]></content>
      <categories>
        
          <category> DeepLearning </category>
        
      </categories>
      <tags>
        
          <tag> deep learning </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[PMI]]></title>
      <url>/nlp/2018/05/24/PMI/</url>
      <content type="text"><![CDATA[PMI (Point Mutual Information)PMI는 BOW models 같은 sparse vector representation에서 semantics를 학습하기 위해 이용되는 방법이다.독립/상관관계확률 이론에서 전체 공간에서의 $p(y)$ 와 $x$ 조건에서의 $p(y\vert x)$ 가 같으면 $x$ 와 $y$ 는 독립이다.                   저녁을 먹었다      저녁을 먹지 않았다      Prob.                  안경을 썼다      100      200      3/12              안경을 쓰지 않았다      300      600      9/12              Prob.      4/12      8/12             서로 양의 상관성이 있으면 $\dfrac{p(x, y)}{p(x) \times p(y)}$ 가 1보다 크다.                   저녁을 먹었다      저녁을 먹지 않았다      Prob.                  안경을 썼다      200      100      3/12              안경을 쓰지 않았다      300      600      9/12              Prob.      5/12      7/12             PMI는 두 경우의 상관성을 파악하는 인덱스이다.  양의 상관관계라면 0보다 큰 값을, 그 반대의 경우엔 0보다 작은 값을 가진다.  값의 방향성에 해석력이 있다.PPMI (Positive PMI)그러나 자연어처리에서의 semantic에서는 음의 상관관계에 큰 의미가 없다. 따라서 양의 상관관계 패턴을 강조하기 위해 0보다 작은 값을 0으로 변환한다.Smoothing PMIPMI (PPMI)는 적은 빈도수에 민감하다. $p(y)$ 가 지나치게 작으면 대부분의 $y$ 가 $x$ 에서 발생할 가능성이 있다. 이를 방지하기 위해 $p(y)$ 에 일정한 값 $\alpha$ 를 더한다. $\alpha$ 는 $x$ 에 따라 다르게 적용되어야 한다.  $p(y\vert x)$ 가 $\alpha$ 이상인 경우에만 PMI를 계산하는 효과가 있다.  $\alpha$ 는 threshold 역할을 한다.Defining contextssemantic을 표현할 대상과 이를 설명하는 정보들을 설정한다.  $x$ 는 semantic을 표현할 대상이다.  $y$ 는 $x$ 를 설명하는 정보 (context)이다.  (Term, context term)을 ($x, y$)로 표현할 수 있다.(word - context) pair는 Word2Vec의 개념과 유사하다. Levy &amp; Goldberg (2014, NIPS)에서 Word2Vec은 PMI 행렬에 차원 축소 기법을 적용한 것과 비슷하다는 사실을 밝혔다.]]></content>
      <categories>
        
          <category> NLP </category>
        
      </categories>
      <tags>
        
          <tag> nlp </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[Corpus-based Chatbots]]></title>
      <url>/nlp/2018/05/22/Corpus-based-Chatbots/</url>
      <content type="text"><![CDATA[  이 글은 stanford의 Dan Jurafsky and James H. Martin가 쓴 Speech and Language Processing (3rd ed. draft) 중 ch 29.Dialog Systems and Chatbots을 참고했습니다.Corpus-based chatbots말뭉치 기반 챗봇은 규칙을 이용하는 대신 인간 간 대화 혹은 인간-기계 간 대화에서의 인간의 반응을 학습한다. 보통 이용되는 말뭉치로는 Twitter, Weibo, 영화 대사 등이 있다.말뭉치 기반 챗봇 시스템에는 두 타입이 있는데 하나는 정보 검색 기반(information retrieval based)이고 다른 하나는 시퀀스 변환 기반의 지도 학습 시스템이다.IR-based chatbotsIR 기반 챗봇의 원리는 사용자의 반응 $X$ 에 대해 말뭉치로부터 적절한 $Y$를 선택해 대응하는 것이다. 이는 말뭉치의 선택과 말뭉치로부터 어떤 것을 선택할지에 달려 있다. IR 기반 챗봇은 두 가지의 간단한 방법을 통해 말뭉치에서 적절한 답변을 선택한다.1) 사용자의 턴과 가장 유사한 턴에 대답 리턴사용자 질의 $q$ 와 대화 말뭉치 $C$ 에 대해 $q$ 와 가장 유사한(코사인 유사도 등) $t$ 턴을 찾고 그에 이어지는 대답을 리턴한다.2) 가장 유사한 턴 리턴사용자 질의 $q​$ 와 대화 말뭉치 $C​$ 에 대해 $q​$ 와 가장 유사한(코사인 유사도 등) $t​$ 를 리턴한다. 즉 사용자의 질의에 바로 그에 맞는 대답을 리턴한다. 좋은 대답은 일정 단어나 문맥 의미를 공유한다는 아이디어이다.각 케이스에서 단어나 임베딩에 대해 유사도 함수가 사용되는데 주로 코사인 유사도가 사용된다. 1)번이 좀 더 직관적인 알고리즘으로 보이지만 실제로는 2)번이 더 잘 동작한다. 대답을 위한 레이어가 추가적인 노이즈를 유발하기 때문이다.IR 기반 챗봇은 사용자의 질의 $q​$ 외에도 추가적인 변수를 사용하고 full IR 랭킹 접근 방식을 사용해 확장할 수 있다. IR 기반 챗봇의 상업적 접근에는 Cleverbot과 Microsoft의 ‘XioaIce’ (Little Bing, 小冰, 작은 얼음) 등이 있다.Sequence to sequence chatbots다른 방법은 사용자의 이전 대화를 시스템의 턴으로 변환하는 것이다. Eliza의 머신러닝 버전이며, 질문을 대답으로 변환하는 것이다. 초기엔 구분 기반의 MT로 발전했으나 어려움을 겪는데, MT에서 입력 문장과 타겟 문장은 잘 매칭되지만 사용자 발화는 대답과 단어나 구문을 공유하지 않을 수도 있기 때문이다.그 대신 seq2seq 모델을 이용한 변환 기반의 대답 생성 모델이 제안되었다. 기본 seq2seq는 실전을 위해 몇 가지 수정을 거쳐야 하는데 기본 seq2seq 모델은 예측 가능하지만 반복적인, 따라서 조금은 멍청한 “I’m OK”나 “I don’t know” 등의 대답을 만들어내는 경향을 띠기 때문이다. 이는 목적 함수를 mutual information objective를 학습하도록 수정하거나 beam 디코더를 좀 더 다양한 대답을 유지하도록 수정하면 된다.또 하나의 문제는 이전 대화의 긴 문맥의 모델링인데 이는 이전 몇 개의 정보를 요약하는 등의 계층적 모델을 사용해 모델이 이전의 대화를 보도록 하면 된다.또한 SEQ2SEQresponse 모델은 단답만 생성해서 여러 턴에 걸쳐 응집된 대화를 생성하는 데는 좋지 않다. 이 점은 적대적 네트워크 같은 기술과 강화 학습을 통해 전체적으로 자연스러운 대화를 생성해내도록 함으로써 해결할 수 있다.Evaluating Chatbots챗봇은 일반적으로 사람이 평가한다. BLEU는 챗봇의 대답과 인간의 대답을 비교하는 데 안 좋은 성능을 보인다. 이는 하나의 턴에 대응 가능한 대답이 매우 많기 때문이다. 대답의 경우의 수가 적고 어휘적으로 겹치는 부분이 있다면 word-overlap metrics가 가장 좋은 평가 성능을 보인다.따라서 챗봇의 평가엔 인간의 개입이 필요하지만 자동 평가를 위한 모델이 제안되고 있다. ADEM은 인간이 그 적절성을 직접 평가한 대화 데이터를 이용했으며 대화 문맥과 시스템 대답의 단어로부터 태깅된 레이블을 예측하도록 학습했다. 또 하나의 패러다임은 adversarial evaluation이다. 이 패러다임의 아이디어는 기계가 만들어낸 대답과 인간이 만들어낸 대답 사이를 학습하는 것이다. 즉, 일반적인 GAN과 같이 시스템이 더 좋은 대답을 만들어낼 수록 그 성능은 개선되는 것이다.]]></content>
      <categories>
        
          <category> NLP </category>
        
      </categories>
      <tags>
        
          <tag> deep learning </tag>
        
          <tag> nlp </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[행렬의 미분 (Jacobian, Hessian)]]></title>
      <url>/machinelearning/2018/05/18/%ED%96%89%EB%A0%AC%EC%9D%98-%EB%AF%B8%EB%B6%84/</url>
      <content type="text"><![CDATA[스칼라를 벡터로 미분하는 경우스칼라를 벡터로 미분하는 경우에는 결과를 열벡터로 표시한다. 이렇게 만들어진 벡터를 그래디언트 벡터(gradient vector)라고 부르며 $\nabla y$ 로 표기한다.2개의 독립 변수를 가지는 함수는 2차원 상에서 countour plot으로 나타낼 수 있다. 이 때 각 위치에서의 그래디언트 벡터를 화살표로 표현한 것을 quiver plot이라고 한다. quiver plot에서 그래디언트 벡터는 다음과 같은 특성을 가진다.  그래디언트 백터의 방향은 함수 곡면의 기울기가 가장 큰 방향, 즉 단위 길이당 함수 값이 가장 크게 증가하는 방향을 가리킨다.  그래디언트 벡터의 방향은 등고선의 방향과 직교한다.  그래디언트 벡터의 크기는 기울기를 의미한다. 즉 벡터의 크기가 클수록 함수 곡면의 기울기가 커진다.행렬 미분 법칙다변수 함수를 미분해 그래디언트를 구할 때는 다음 두 가지 법칙이 유용하게 쓰인다.1. 선형 모형선형 모형을 미분하면 가중치 벡터가 된다.2. 이차 형식 (Quadratic Form)이차 형식을 미분하면 행렬과 벡터의 곱으로 나타난다.벡터를 스칼라로 미분하는 경우벡터를 스칼라로 미분하는 경우에는 결과를 행벡터로 표시한다.벡터를 벡터로 미분하는 경우함수의 종속 변수와 독립 변수가 모두 벡터(다차원) 데이터인 경우에는 독립 변수와 종속 변수 각각에 대해 모두 미분값이 존재한다. 따라서 도함수는 행렬 형태가 된다. 이렇게 만들어진 도함수 행렬을 자코비안 행렬(Jacobian Matrix) 라고 한다.다변수 함수의 2차 도함수는 그래디언트 벡터를 독립 변수 벡터로 미분한 것이므로 다음과 같은 행렬로 나타낼 수 있다. 이러한 행렬을 헤시안 행렬(Hessian Matrix) 이라고 한다. 헤시안 행렬은 일반적으로 대칭 행렬이다.ML/DL에서 쓰이는 SGD, Momemtum, 네스테로프, AdaGrad, RMSProp, Adam 등은 모두 자코비안(1차 편미분)에만 의존한다. 헤시안 기반의 최적화 알고리즘들(CG &amp; BFGS 등)이 있지만 이런 알고리즘들은 DNN에 적용하기에는 어렵다. 하나의 출력마다 $n^2$ 개의 2차 편미분을 계산해야 하기 때문이다. 수만, 수십만 개의 파라미터에 대해 헤시안 계산을 적용하기에는 메모리 용량이 한계를 가지고 또 너무 느리다.]]></content>
      <categories>
        
          <category> MachineLearning </category>
        
      </categories>
      <tags>
        
          <tag> machine learning </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[[Algorithm] 재귀와 완전 탐색]]></title>
      <url>/algorithm/2018/05/18/Algorithm-%EC%9E%AC%EA%B7%80%EC%99%80-%EC%99%84%EC%A0%84-%ED%83%90%EC%83%89/</url>
      <content type="text"><![CDATA[  참고: 프로그래밍 대회에서 배우는 알고리즘 문제 해결 전략 (구종만 저)  http://grayt.tistory.com/16이 글의 코드는 원저의 C++ 코드를 파이썬으로 옮긴 코드입니다.컴퓨터의 강점은 처리 속도이기 때문에 작은 입력이 주어졌을 때 가능한 경우의 수를 모두 탐색하는 완전 탐색을 유용하게 사용할 수 있다. 이 때 재귀 호출을 이용하면 특정 조건을 만족하는 조합을 모두 생성하는 코드를 쉽게 작성할 수 있기 때문에 완전 탐색을 구현하는 데 유용하다.예제: 중첩 반복문 대체하기0번부터 차례대로 번호가 매겨진 n개의 원소 중 네 개를 고르는 모든 경우를 출력하는 코드를 작성해보자. 가장 간단한 풀이는 4중 for 문을 작성하는 것이다.array = []n = 7for i in range(n):    for j in range(i+1, n):        for k in range(j+1, n):            for l in range(k+1, n):                array.append([i, j, k, l])from pprint import pprintpprint(array[:5])# [[0, 1, 2, 3], [0, 1, 2, 4], [0, 1, 2, 5], [0, 1, 2, 6], [0, 1, 3, 4]]하지만 골라야 하는 입력 개수가 늘어날 때마다 for 문을 늘려야하기 때문에 입력에 유연하게 대처할 수 없다. 이 때 재귀를 사용하면 입력에 유연하게 대처할 수 있다.def pick(n, picked, to_pick):    """    n: 전체 원소의 수    picked: 지금까지 고른 원소들의 번호    to_pick: 더 고를 원소의 수    """    # 기저 조건: 더 고를 원소가 없을 때 고른 원소들을 출력    if to_pick == 0:        print(picked)        return        if not picked:        smallest = 0    else:        smallest = picked[-1] + 1        for _next in range(smallest, n):        picked.append(_next)        pick(n, picked, to_pick-1)        picked.pop(-1)        print(pick(7, [], 4))&gt;&gt;&gt;[0, 1, 2, 3][0, 1, 2, 4][0, 1, 2, 5][0, 1, 2, 6]...[2, 4, 5, 6][3, 4, 5, 6]None예제: 보글 게임보글은 5X5 크기의 알파벳 격자를 가지고 하는 게임이다. 게임의 목적은 상하, 좌우, 대각으로 인접한 칸들의 글자들을 이어 단어를 찾아내는 것이다. 한 글자가 두 번 이상 사용될 수도 있다. 주어진 칸에서 시작해 특정 단어를 찾을 수 있는지 확인하는 문제를 풀어본다. 함수는 다음과 같을 것이다. “$hasWord(y, x, word)$  = 보글 게임판의 (y, x)에서 시작하는 단어 word의 존재 여부를 리턴한다.”간단한 방법은 완전 탐색을 이용해 단어를 찾아낼 때까지 인접한 모든 칸을 하나씩 시도해 보는 것이다. 그 중 한칸에서라도 단어를 찾을 수 있으면 성공이고 어느 칸을 선택하더라도 답을 찾을 수 없다면 실패가 된다.문제의 분할$hasWord()$ 가 하는 일을 가장 자연스럽게 조각내는 방법은 각 글자를 하나의 조각으로 만드는 것이다. 먼저 시작 위치의 글자가 단어의 첫 글자와 일치하면 원래 단어에서 첫 글자를 뗀 word[1:]을 격자에서 다시 찾는다. 인접한 여덟 경우를 모두 탐색한다.기저 사례의 선택더 이상의 탐색 없이 간단히 답을 낼 수 있는 다음 경우들으 기저 사례로 선택한다.  위치 (y, x)에 있는 글자가 원하는 단어의 첫 글자가 아닌 경우 항상 실패  (1번이 아닌 경우) 원하는 단어가 한 글자인 경우 항상 성공단, 위의 두 조건의 순서는 바뀌면 안 된다.  또한 입력값이 격자의 범위를 벗어난 경우도 기저 사례로 선택해 가장 먼저 처리한다.구현dx = np.array([-1, -1, -1, 1, 1, 1, 0, 0])dy = np.array([-1, 1, 0, -1, 0, 1, -1, 1])def has_word(y, x, word):    # 기저 1: board의 범위를 벗어나는 경우    if (y &gt; len(board)) and (x &gt; len(board[0])):        return False    # 기저 2: 첫 글자가 좌표의 위치의 글자와 다를 경우    if board[y][x] != word[0]:        return False    # 기저 3: 글자가 1개면 더 이상 비교할 게 남아 있지 않음    if len(word) == 1:        return True    for direction in range(8):        next_y = y + dy[direction]        next_x = x + dx[direction]                # 여기까지 왔다는 것은 이미 첫 글자가 일치한 경우이므로        # 그 다음 글자부터 다시 시작        # 한 방향으로 무식하게 파고 들어가는 완전 탐색        if has_word(next_y, next_x, word[1:]):            return True    return False실행 결과는 다음과 같다.board = np.array([['U', 'R', 'L', 'P', 'M'],                   ['X', 'P', 'R', 'E', 'T'],                   ['G', 'I', 'A', 'E', 'T'],                   ['X', 'T', 'N', 'Z', 'Y'],                   ['X', 'O', 'Q', 'R', 'S']])print(has_word(2, 0, 'GIRL'))# True]]></content>
      <categories>
        
          <category> Algorithm </category>
        
      </categories>
      <tags>
        
          <tag> algorithm </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[[Algorithm] 삽입 정렬]]></title>
      <url>/algorithm/2018/05/16/Algorithm-%EC%82%BD%EC%9E%85-%EC%A0%95%EB%A0%AC/</url>
      <content type="text"><![CDATA[  참고: 프로그래밍 대회에서 배우는 알고리즘 문제 해결 전략 (구종만 저)이 글의 코드는 원저의 C++ 코드를 파이썬으로 옮긴 코드입니다.삽입 정렬의 작동 방식  전체 배열 중 정렬되어 있는 부분 배열에 새 원소를 끼워넣는 일을 반복  배열을 순회하면서 그 앞의 원소가 해당 원소보다 크면 두 원소의 위치를 교환  전체 시간 복잡도는 역순으로 정렬된 배열이 주어질 경우 for문에서 $O(N)$, while문에서 $O(N)$이 되기 때문에 $O(N^2)$def insertion_sort(array):    for i in range(len(array)):        j = i        while (j &gt; 0) and (array[j-1] &gt; array[j]):            # 원저의 C++에서는 swap(array[j-1], array[j]);            array[j-1], array[j] = array[j], array[j-1]             j -= 1    return arrayarray = [1, 4, 7, 11, 5, 6, 12, 9]print(insertion_sort(array))# [1, 4, 5, 6, 7, 9, 11, 12]]]></content>
      <categories>
        
          <category> Algorithm </category>
        
      </categories>
      <tags>
        
          <tag> algorithm </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[[Algorithm] 계산 복잡도 클래스: P, NP, NP완비]]></title>
      <url>/algorithm/2018/05/15/Algorithm-%EA%B3%84%EC%82%B0-%EB%B3%B5%EC%9E%A1%EB%8F%84-%ED%81%B4%EB%9E%98%EC%8A%A4-P-NP-NP%EC%99%84%EB%B9%84/</url>
      <content type="text"><![CDATA[  참고: 프로그래밍 대회에서 배우는 알고리즘 문제 해결 전략 (구종만 저)문제의 특성계산 복잡도 이론은 각 문제의 특성을 공부하는 학문이다. 다음 두 문제에 대해 살펴보자.  정렬 문제: 주어진 $N$개의 정수를 정렬한 결과는 무엇인가?  부분 집합 합(subset sum) 문제: $N$개의 수가 있을 때 이 중 몇 개를 골라내서 그들의 합이 $S$가 되도록 할 수 있는가?이 때 각 문제의 난이도는 문제를 풀 때의 난이도가 아니다. 계산 복잡도 이론에서 문제의 난이도는 문제를 풀 때의 난이도가 아니라 해당 문제를 해결하는 빠른 알고리즘이 있느냐를 나타낸다. 빠른 알고리즘이 있는 문제는 계산적으로 쉽고, 그렇지 않은 문제는 계산적으로 어렵다고 말한다. 그렇다면 ‘빠른’ 알고리즘이란 무엇일까? 일반적으로 다항 시간 알고리즘이나 그보다 빠른 알고리즘들만을 ‘빠르다’고 말한다.계산 복잡도 이론에서 이렇게 다항 시간 알고리즘이 존재하는 문제들의 집합을 P 문제라고 부른다. P 문제처럼 같은 성질을 갖는 문제들을 모아놓은 집합을 계산 복잡도 클래스(complexity class)라고 부른다. 그 중 중요한 것이 P, NP 문제이다.난이도의 함정그러나 어떤 문제를 다항 시간에 풀 수 있음을 증명하기는 쉽지만, 풀 수 없음을 보이기란 어렵다. 따라서 다항 시간 알고리즘이 존재하는 문제와 존재하지 않는 문제로 문제들을 구분하기엔 어렵다. 계산 복잡도에서 흔히 말하는 ‘어려운 문제’들은 다음과 같이 정의된다.  정말 어려운 문제를 잘 골라서 이것을 어려운 문제의 기준으로 삼는다.  앞으로는 기준 문제만큼 어렵거나 그보다 어려운 문제들 즉, ‘기준 이상으로 어려운 문제들’만을 어렵다고 부른다.하지만 이 기준에 따라 문제를 분류하려면 난관에 부딪힌다. 주어진 문제가 기준 이상으로 어려운지 판정하기가 쉽지 않기 때문이다. 문제 A가 문제 B 이상으로 어렵다고 말하려면 A를 푸는 가장 빠른 알고리즘이 B를 푸는 가장 빠른 알고리즘 이상의 시간이 걸려야 한다. 그러나 대부분의 경우 우리는 문제를 푸는 가장 빠른 알고리즘이 무엇인지 모른다.계산 복잡도 이론에서는 두 문제의 난이도를 비교하기 위해 환산(reduction)이라는 기법을 사용한다. 이는 한 문제를 다른 문제로 바꿔서 푸는 기법이다. B의 입력을 변형해 A의 입력으로 바꾸는 환산 알고리즘이 존재하면 A를 푸는 가장 빠른 알고리즘으로 B를 해결하는 알고리즘을 만들 수 있다. 환산 알고리즘의 실행 시간이 무시할 수 있을 정도로 빠르다고 하면 결합된 알고리즘은 A를 푸는 가장 빠른 알고리즘과 같은 시간이 걸릴 것이다. B를 푸는 가장 빠른 알고리즘은 이 결합된 알고리즘과 같거나 더 빠를 것이다. 결국 B를 푸는 가장 빠른 알고리즘은 A를 푸는 가장 빠른 알고리즘과 같거나 더 빠를 수 밖에 없다. 즉 A가 B 이상으로 어렵다는 것이다.NP 문제, NP 난해 문제문제의 난이도를 비교할 수 있으니 어려운 문제의 기준을 정해야 한다. 이 때 어려운 문제의 기준이 되는 것이 바로 SAT 문제(satisfiability problem)이다. SAT 문제란 $N$개의 boolean 값 변수로 구성된 논리식을 참으로 만드는 변수 값들의 조합을 찾는 문제이다. SAT 문제는 모든 NP 문제 이상으로 어렵다.NP 문제란 답이 주어졌을 때 이것이 정답인지를 다항 시간 내에 확인할 수 있는 문제를 의미한다. 모든 P 문제들은 NP 문제에도 포함된다. SAT가 모든 NP 문제 이상으로 어렵다는 것은 SAT를 다항 시간에 풀 수 있으면 NP 문제들을 전부 다항 시간에 풀 수 있다는 말과 동일하다. 이런 속성을 갖는 문제들의 집합을 NP-난해(NP-Hard) 문제라고 부른다. NP-난해 문제들은 아직 다항 시간에 푸는 방법이 발견되지 않았다.NP-난해 문제이면서 NP인 문제들을 NP-완비(완전) 문제라고 한다.P=NPP=NP 문제는 P와 NP가 같은지를 확인하는 문제이다. NP-난해 문제 중 하나를 다항 시간에 풀 수 있다면 이 알고리즘을 이용해 NP에 속한 무든 문제를 다항 시간에 풀 수 있다. 이 경우 NP에 속한 모든 문제를 다항 시간에 해결할 수 있으므로 P=NP임을 알 수 있다. 반대로 NP문제 중 하나를 골라 다항 시간에 푸는 방법이 없음을 증명하면 P$\neq$NP임을 보일 수 있다. 이 문제는 아직까지 미해결 문제이다.]]></content>
      <categories>
        
          <category> Algorithm </category>
        
      </categories>
      <tags>
        
          <tag> algorithm </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[[Algorithm] 시간 복잡도와 수행 시간]]></title>
      <url>/algorithm/2018/05/15/Algorithm-%EC%8B%9C%EA%B0%84-%EB%B3%B5%EC%9E%A1%EB%8F%84%EC%99%80-%EC%88%98%ED%96%89-%EC%8B%9C%EA%B0%84/</url>
      <content type="text"><![CDATA[  참고: 프로그래밍 대회에서 배우는 알고리즘 문제 해결 전략 (구종만 저)이 글의 코드는 원저의 C++ 코드를 파이썬으로 옮긴 코드입니다.1차원 배열에서 연속된 부분 구간 중 그 합이 최대인 구간을 찾는 문제는 여러 알고리즘으로 해결할 수 있다. 시간 복잡도가 서로 다른 여러 알고리즘을 구현하고 각 알고리즘의 수행 시간이 어떻게 다른지 확인해본다.테스트 배열로는 $A = \begin{bmatrix} -7, 4, -3, 6, 3, -8, 3, 4 \end{bmatrix}$ 을 사용하며, 해답은 $\begin{bmatrix}4, -3, 6, 3 \end{bmatrix}$ 일 때 10이다.1. 실행 시간 $O(N^3)$A = [-7, 4, -3, 6, 3, -8, 3, 4]# N**2개의 후보 구간을 검사하고# 각 구간의 합을 구하는데 N시간이 걸리는 알고리즘def inefficient_max_sum(array):    n = len(array)    max_sum = 0    for i in range(n):        for j in range(n):            _sum = 0            for k in range(i, j):                _sum += array[k]            max_sum = max(max_sum, _sum)    return max_sumprint(inefficient_max_sum(A))# 102. 실행 시간 $O(N^2)$def better_max_sum(array):    n = len(array)    max_sum = 0    for i in range(n):        _sum = 0        # 구간 array[i, j]의 합을 구한다        for j in range(i, n):            _sum += array[j]            max_sum = max(max_sum, _sum)    return max_sumprint(better_max_sum(A))# 103. 실행 시간 $O(N\log{N})$분할 정복과 탐욕 알고리즘을 사용하면 실행 시간을 $O(N\log{N})$으로 줄일 수 있다. 1) 입력받은 배열을 반으로 잘라 왼쪽 배열과 오른쪽 배열로 나눈다. 2) 최대 합 부분 구간은 두 배열 중 하나에 속해 있을 수도 있고, 두 배열 사이에 걸쳐 있을 수도 있다. 3) 이 때 각 경우의 답을 재귀와 탐욕법을 이용해 계산하면 분할 정복 알고리즘이 된다.# 분할 정복, 탐욕 알고리즘 사용def fast_max_sum(array, lo, hi):    n = len(array)     # 길이가 1이면 그 값을 그대로 리턴    if lo == hi:        return array[lo]        # 배열을 반으로 나눔    mid = (lo + hi) // 2    # 왼쪽/오른쪽 배열의 합, 합을 저장할 초깃값    left, right, _sum = 0, 0, 0        # 두 부분에 모두 걸쳐 있는 최대 합 구간을 찾는다.    # 이 구간은 array[i, mid]와 array[mid+1, j] 형태를 갖는 구간의 합으로 이루어진다.    # array[i, mid] 형태를 갖는 최대 구간을 찾는다.    for i in range(mid, 0, -1):        _sum += array[i]        left = max(left, _sum)        # array[mid+1, j] 형태를 갖는 최대 구간을 찾는다.    _sum = 0    for j in range(mid+1, hi):        _sum += array[j]        right = max(right, _sum)        # 최대 구간이 두 배열 중 하나에만 속해 있는 경우의 답을 재귀로 찾는다.    single = max(fast_max_sum(array, lo, mid),                  fast_max_sum(array, mid+1, hi))        # 두 경우 중 최대치를 리턴    return max(left + right, single)print(fast_max_sum(A, 1, 5))# 104. 실행 시간 $O(N)$동적 계획법을 사용하면 선형 시간에 문제를 풀 수 있다. 배열 $A[i]$를 오른쪽 끝으로 갖는 구간의 최대 합을 리턴하는 함수 $max  At(i)$를 정의해보자. 이 때, $A[i]$ 에서 끝나는 최대 합 부분 구간은 항상 $A[i]$ 하나만으로 구성되어 있거나, $A[i-1] $을 오른쪽 끝으로 갖는 최대 합 부분 구간의 오른쪽에 $A[i]$ 를 붙인 형태로 구성되어 있음을 증명할 수 있다. 따라서 $maxAt()$ 를 다음과 같은 점화식으로 표현할 수 있다.def fastest_max_sum(array):    n = len(array)    max_sum, psum = 0, 0    for i in range(n):        psum = max(psum, 0) + array[i]        max_sum = max(psum, max_sum)    return max_sumprint(fastest_max_sum(A))# 101초 안에 처리할 수 있는 최대 입력 크기는 각 알고리즘에 따라 다음과 같이 변한다.  $O(N^3)$: 크기 2560인 입력까지를 1초 안에 풀 수 있다. $2560^3$은 대략 160억이다.  $O(N^2)$: 크기 40960인 입력까지를 1초 안에 풀 수 있다. $40960^2$은 대략 16억이다.  $O(N\log{N})$: 크기가 대략 2천만인 입력까지를 1초 안에 풀 수 있다. 이 때 $N\log{N}$은 대략 5억이다.  $O(N)$: 크기가 대략 1억 6천만인 입력까지 1초 안에 풀 수 있다.]]></content>
      <categories>
        
          <category> Algorithm </category>
        
      </categories>
      <tags>
        
          <tag> algorithm </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[CART 훈련 알고리즘]]></title>
      <url>/machinelearning/2018/05/11/CART-%ED%9B%88%EB%A0%A8-%EC%95%8C%EA%B3%A0%EB%A6%AC%EC%A6%98/</url>
      <content type="text"><![CDATA[CART 훈련 알고리즘  참고: 핸즈온 머신러닝 (오렐리앙 제롱 저, 박해선 역)scikit-learn은 decision tree를 훈련시키기 위해 CART (Classification And Regression Tree) 알고리즘을 사용한다. 이 알고리즘의 아이디어는 간단하다. 먼저 train set을 하나의 특성 $k$ 의 임곗값 $t_k$ 를 사용해 두 개의 서브셋으로 나눈다. 이 때 $k$ 와 $t_k$ 는 가장 순수한 서브셋으로 나눌 수 있는 ($k$, $t_k$) 로 찾는다. 이 알고리즘의 최소화해야 하는 비용 함수는 다음과 같다.  $G_{\text{left/right}}$: 왼쪽/오른쪽 서브셋의 불순도  $m_{\text{left/right}}$: 왼쪽/오른쪽 서브셋의 샘플 수train set이 성공적으로 나눠지면 같은 방식으로 서브셋을 나누는 과정을 반복한다. 이 과정은 최대 깊이가 되거나 불순도를 줄이는 분할을 찾을 수 없을 때 중지된다.CART 알고리즘은 탐욕 알고리즘이다. 납득할만한 솔루션을 만들어내지만 최적의 솔루션을 보장하지는 않는다.]]></content>
      <categories>
        
          <category> MachineLearning </category>
        
      </categories>
      <tags>
        
          <tag> machine learning </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[Stanford Pos Tagger를 이용한 POS Tagging]]></title>
      <url>/nlp/2018/04/05/NLP-POS-Tagging-%ED%92%88%EC%82%AC-%ED%83%9C%EA%B9%85/</url>
      <content type="text"><![CDATA[  Stanford Pos Tagger를 다운받고 modelr과 jar파일의 경로를 지정한 뒤, pos_tagger로 불러온다.  Stanford Pos Tagger Download Link: Stanford Log-linear Part-Of-Speech Taggerfrom nltk.tag import StanfordPOSTaggerfrom nltk.tokenize import word_tokenizeSTANFORD_POS_MODEL_PATH = "압축 푼 디렉토리/stanford-postagger-full-2018-02-27/models/english-bidirectional-distsim.tagger"STANFORD_POS_JAR_PATH = "압축 푼 디렉토리/stanford-postagger-full-2018-02-27/stanford-postagger-3.9.1.jar"pos_tagger = StanfordPOSTagger(STANFORD_POS_MODEL_PATH, STANFORD_POS_JAR_PATH)  분석할 text를 불러오고, word_tokenize로 품사 태깅한다.  Article Source: Zuckerberg: Facebook made a “huge mistake” — but I can fix ittext = """Facebook CEO Mark Zuckerberg acknowledged a range of mistakes on Wednesday, including allowing most of its two billion users to have their public profile data scraped by outsiders. However, even as he took responsibility, he maintained he was the best person to fix the problems he created."""tokens = word_tokenize(text)print(tokens)print()print(pos_tagger.tag(tokens))결과는 다음과 같다.  품사 태깅 약어 정보: Alphabetical list of part-of-speech tags used in the Penn Treebank Project['Facebook', 'CEO', 'Mark', 'Zuckerberg', 'acknowledged', 'a', 'range', 'of', 'mistakes', 'on', 'Wednesday', ',', 'including', 'allowing', 'most', 'of', 'its', 'two', 'billion', 'users', 'to', 'have', 'their', 'public', 'profile', 'data', 'scraped', 'by', 'outsiders', '.', 'However', ',', 'even', 'as', 'he', 'took', 'responsibility', ',', 'he', 'maintained', 'he', 'was', 'the', 'best', 'person', 'to', 'fix', 'the', 'problems', 'he', 'created', '.'][('Facebook', 'NNP'), ('CEO', 'NNP'), ('Mark', 'NNP'), ('Zuckerberg', 'NNP'), ('acknowledged', 'VBD'), ('a', 'DT'), ('range', 'NN'), ('of', 'IN'), ('mistakes', 'NNS'), ('on', 'IN'), ('Wednesday', 'NNP'), (',', ','), ('including', 'VBG'), ('allowing', 'VBG'), ('most', 'JJS'), ('of', 'IN'), ('its', 'PRP$'), ('two', 'CD'), ('billion', 'CD'), ('users', 'NNS'), ('to', 'TO'), ('have', 'VB'), ('their', 'PRP$'), ('public', 'JJ'), ('profile', 'NN'), ('data', 'NNS'), ('scraped', 'VBN'), ('by', 'IN'), ('outsiders', 'NNS'), ('.', '.'), ('However', 'RB'), (',', ','), ('even', 'RB'), ('as', 'IN'), ('he', 'PRP'), ('took', 'VBD'), ('responsibility', 'NN'), (',', ','), ('he', 'PRP'), ('maintained', 'VBD'), ('he', 'PRP'), ('was', 'VBD'), ('the', 'DT'), ('best', 'JJS'), ('person', 'NN'), ('to', 'TO'), ('fix', 'VB'), ('the', 'DT'), ('problems', 'NNS'), ('he', 'PRP'), ('created', 'VBD'), ('.', '.')]  명사와 동사만 따로 떼어서 봐도 기사의 맥락을 이해할 수 있다.noun_and_verbs = []for token in pos_tagger.tag(tokens):    if token[1].startswith("V") or token[1].startswith("N"):        noun_and_verbs.append(token[0])print(', '.join(noun_and_verbs))Facebook, CEO, Mark, Zuckerberg, acknowledged, range, mistakes, Wednesday, including, allowing, users, have, profile, data, scraped, outsiders, took, responsibility, maintained, was, person, fix, problems, created]]></content>
      <categories>
        
          <category> NLP </category>
        
      </categories>
      <tags>
        
          <tag> machine learning </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[교차 검증의 정보 누설]]></title>
      <url>/machinelearning/2018/04/01/%EA%B5%90%EC%B0%A8-%EA%B2%80%EC%A6%9D%EC%9D%98-%EC%A0%95%EB%B3%B4-%EB%88%84%EC%84%A4/</url>
      <content type="text"><![CDATA[  The Elements of Statistical Learning (Hastie, Springer, 2016)파이썬 라이브러리를 활용한 머신러닝 (안드레아스 뮐러 저, 박해선 역, 한빛미디어, 2017)Improper Preprocessing교차 검증에서 검증 폴드 데이터의 정보가 모델 구축 과정에 누설되면 교차 검증에서 낙관적인 결과가 만들어진다. 아래 그림에서는 파라미터 선택을 위해 scaler.fit과 SVC.predict가 모두 검증 폴드를 사용하고 있다. 하지만 모델의 성능을 평가할 때는 scaler.fit이 테스트 세트에 적용되지 않는다. (아래와 같은 문제를 해결하기 위해 Pipeline을 사용한다.)[Pipeline 사용 예시]# MinMaxScaler와 SVC를 Pipeline으로 연결하고# GridSearchCV에 Pipeline을 적용pipe = Pipeline([("Scaler", MinMaxScaler()), ("svm", SVC())])pipe.fit(X_train, y_train)param_grid = {"svm__C": [0.001, 0.01, 0.1, 1, 10, 100],               "svm__gamma": [0.001, 0.01, 0.1, 1, 10, 100]}grid = GridSearchCV(pipe, param_grid=param_grid, cv=5)grid.fit(X_train, y_train)교차 검증에서 정보 누설의 영향무작위로 생성된 X와 y에는 아무런 관계가 없다. (독립)# 정규분포로부터 독립적으로 추출한 10,000개의 특성을 가진 샘플 100개rnd = np.random.RandomState(seed=0)X = rnd.normal(size=(100, 10000))y = rnd.normal(size=(100,))아래와 같이 피처를 선택하고 학습을 시키면 다음과 같은 점수를 얻는다.# SelectPercentile로 유용한 피처를 선택하고 교차 검증을 사용해 Ridge 회귀 평가from sklearn.feature_selection import SelectPercentile, f_regressionselect = SelectPercentile(score_func=f_regression, percentile=5).fit(X, y)X_selected = select.transform(X)print("X_selected.shape: {}".format(X_selected.shape)) # (100, 500)from sklearn.model_selection import cross_val_scorefrom sklearn.linear_model import Ridgeprint("CV Score (Ridge): {:.2f}".format(    np.mean(cross_val_score(Ridge(), X_selected, y, cv=5))))CV Score (Ridge): 0.91$R^2​$가 0.91로 매우 좋은 모델이라고 생각할 수 있지만, 데이터셋을 무작위로 만들었기 때문에 불가능한 일이다. 교차 검증 밖에서 특성을 선택했기 때문에 훈련과 테스트 폴드 양쪽에 연관된 특성이 찾아질 수 있다. 테스트 폴드에서 유출된 정보는 매우 중요한 역할을 하기 때문에 비현실적으로 높은 결과가 나왔다. 이 결과를 Pipeline을 이용한 교차 검증과 비교하면 다음과 같다.pipe = Pipeline([("select", SelectPercentile(score_func=f_regression,                                              percentile=5)),                  ("ridge", Ridge())])print("CV score (Pipeline): {:.2f}".format(    np.mean(cross_val_score(pipe, X, y, cv=5))))CV score (Pipeline): -0.25이번에는 $R^2$가 음수로 성능이 매우 나쁘 모델임을 나타낸다. Pipeline을 사용했기 때문에 특성 선택이 교차 검증 반복 안으로 들어갔다. 즉, 훈련 폴드를 사용해서만 특성이 선택되었고 테스트 폴드는 사용되지 않았다는 뜻이다. 특성 선택 단계에서 타깃값과 연관된 훈련 폴드의 특성을 찾았지만 전체 데이터가 무작위로 만들어졌으므로 테스트 폴드의 타깃과는 연관성이 없다. 이는 특성 선택 단계에서 일어나는 정보 누설을 막는 것이 모델의 성능을 평가하는 데 큰 차이를 만든다는 것을 보여준다.]]></content>
      <categories>
        
          <category> MachineLearning </category>
        
      </categories>
      <tags>
        
          <tag> machine learning </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[Mac High Sierra에서 XGBoost 설치하기]]></title>
      <url>/machine%20learning/2018/03/20/Mac-High-Sierra%EC%97%90%EC%84%9C-xgboost-%EC%84%A4%EC%B9%98%ED%95%98%EA%B8%B0/</url>
      <content type="text"><![CDATA[2018. 03. 20Mac High Sierra에서 XGBoost 설치하기XGBoost  분산형 그래디언트 부스팅  여러 개의 결정 트리를 묶어 강력한 모델을 만드는 또다른 앙상블 방법  랜덤포레스트와 달리 이전 트리의 오차를 보완하는 방식으로 순차적으로 트리를 만듦  무작위성이 없음  메모리를 적게 사용하며 예측도 빠름 (트리가 많이 추가될수록 성능이 좋아짐)  XGBoost는 빠르고, 튜닝하기 쉬움  강력하고 널리 사용되는 모델 중 하나Installation2018.03.20 기준 XGBoost 최신 버전: 0.7.0[사용 중인 프로그램과 환경]OS - macOS High Sierra 10.13.3Xcode 9.2brew 1.5.11gcc 7.3.0.1 (2018.03.20 기준)python 3.6.3 anaconda3 custom (conda 4.4.3)  git에서 해당 프로젝트를 클론$ git clone --recursive https://github.com/dmlc/xgboost.git  gcc 컴파일러 설치 (2018.03.20 기준으로 7.3.0.1 버전이 설치된다.)$ brew install gcc --without-multilib  최신버전의 gcc를 사용하고 있다면 make 파일을 변경해야 한다. 이렇게 해야 멀티스레드를 지원한다.$ cd xgboost$ cp make/config.mk ./config.mk$ make -j4make -j4 명령에서  clang: error: unsupported option '-fopenmp' 오류를 마주하게 되는데, 이를 해결하는 방법은 다음과 같다.참고 문서: https://github.com/ppwwyyxx/OpenPano/issues/16 (OpenMP 오류 해결)  clang: error: unsupported option '-fopenmp' 오류 해결 방법아래의 명령은 디렉토리를 따라가면서 현재 설치된 버전에 맞게 실행하면 된다. 아래 명령은 XGBoost 디렉토리가 아닌, 루트 디렉토리에서 실행했다.$ export CXX=/usr/local/Cellar/gcc/7.3.0_1/bin/g++-7$ brew install eigen  아래 명령으로 XGBoost를 빌드한다../build.sh다음 메시지가 나오면 성공이다.make: Nothing to be done for `all'.Successfully build multi-thread xgboost  마지막으로 다음 명령으로 XGBoost 설치 완료한다.$ pip install -e python-package다음 메시지가 나오면 성공이다.Installing collected packages: xgboostRunning setup.py develop for xgboostSuccessfully installed xgboostJupyter Notebook에서도 import xgboost가 잘 동작한다.import pippip.main(['install', 'xgboost'])Requirement already satisfied: xgboost in /Users/sunwoongkim/xgboost/python-packageRequirement already satisfied: numpy in /Users/sunwoongkim/anaconda3/lib/python3.6/site-packages (from xgboost)Requirement already satisfied: scipy in /Users/sunwoongkim/anaconda3/lib/python3.6/site-packages (from xgboost)설치 기본 사항 참고  오늘 코드: http://corazzon.github.io/xgboost-install-mac-osx  XGBoost 공식 문서: http://xgboost.readthedocs.io/en/latest/build.html]]></content>
      <categories>
        
          <category> Machine Learning </category>
        
      </categories>
      <tags>
        
          <tag> machine learning </tag>
        
          <tag> python </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[프로그래밍 언어 (컴파일러/인터프리터 언어)]]></title>
      <url>/python/2018/03/18/%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%B0%8D-%EC%96%B8%EC%96%B4-%EC%BB%B4%ED%8C%8C%EC%9D%BC%EB%9F%AC-%EC%9D%B8%ED%84%B0%ED%94%84%EB%A6%AC%ED%84%B0-%EC%96%B8%EC%96%B4/</url>
      <content type="text"><![CDATA[  출처: 컴퓨터 사이언스 부트캠프 with 파이썬 (양태환, 길벗)컴파일러 언어와 인터프리터 언어는 컴파일 타임이 있느냐 없느냐 즉, 소스 코드를 분석하는 시점과 입력 데이터를 받는 시점이 언제인지에 따라 나뉜다.1. C: 컴파일러 언어 분석  소스 코드를 컴파일  목적 코드(object code)인 기계어로 된 인스트럭션 생성  링커(linker)는 필요한 라이브러리를 가져오고 여러 개의 목적 파일을 함께 묶어 실행 파일(executable file)을 생성  소스 코드를 분석하는 컴파일 타임(compile time)과 실제 데이터를 받아 출력하는 런타임(run time)일 분리되어 있음2. Python: 인터프리터 언어 분석  소스 코드를 컴파일해 바이트 코드(byte code)를 생성  바이트 코드가 생성된 후에는 PVM(Python Virtual Machine)에서 바이트 코드를 해석하여 프로그램을 실행  소스 코드를 분석하는 컴파일 타임이 따로 없고 실행과 동시에 분석을 시작 (소스 코드와 입력 데이터가 같은 시점에 삽입됨)3. Python: 소스 코드부터 실행까지3-1. 컴파일러  일반적인 컴파일러는 렉서(lexer)와 파서(parser)로 구성  소스 코드는 렉서를 거치며 여러 개의 토큰으로 변경됨  파서는 토큰을 분석해 분석 트리(parse tree)를 구성  코드 생성(code generation): 분석 트리가 만들어지만 이를 이용해 목적 코드가 생성3.2. Python의 바이트 코드 생성 과정  소스 코드 $\rightarrow$ 분석 트리  분석 트리 $\rightarrow$ 추상 구문 트리  심벌 테이블 생성  추상 구문 트리 $\rightarrow$ 바이트 코드# 예제 코드 test.py# 두 인자를 더해 리턴하는 함수 func()와 두 전역변수 a, b# a와 b를 받아 func()를 호출하고, 그 값을 c에 저장한 뒤 print() 함수로 출력def func(a, b):    return a + ba = 10b = 20c = func(a, b)print(c)from tokenize import tokenizefrom io import BytesIOs = open('test.py').read()g = tokenize(BytesIO(s.encode("utf-8")).readline)for token in g:    print(token)TokenInfo(type=59 (BACKQUOTE), string='utf-8', start=(0, 0), end=(0, 0), line='')TokenInfo(type=1 (NAME), string='def', start=(1, 0), end=(1, 3), line='def func(a, b):\n')TokenInfo(type=1 (NAME), string='func', start=(1, 4), end=(1, 8), line='def func(a, b):\n')----------- 중략 -----------TokenInfo(type=53 (OP), string=')', start=(8, 7), end=(8, 8), line='print(c)\n')TokenInfo(type=4 (NEWLINE), string='\n', start=(8, 8), end=(8, 9), line='print(c)\n')TokenInfo(type=0 (ENDMARKER), string='', start=(9, 0), end=(9, 0), line='')이렇게 얻어진 토큰으로 분석 트리를 만든 다음, 추상 구문 트리로 변형# 추가: 아래 명령을 사용해서 Python에서 사용하는 토큰 종류를 확인할 수 있다.import tokentoken.tok_name3.2.1. 추상 구문 트리  추상 구문 트리(Abstract Syntax Tree, AST): 소스 코드의 구조를 나타내는 자료 구조  추상 구문 트리를 바탕으로 심벌 테이블, 바이트 코드를 생성할 수 있음import astnode = ast.parse(s, "test.py", "exec") # 노드를 생성하고g = ast.walk(node) # walk 함수를 이용하면 트리의 모든 노드를 순회할 수 있는 제너레이터를 얻을 수 있음print(next(g)) # 제너레이터를 만든 다음 next를 통해 노드를 하나씩 획득print(next(g))print(next(g))&lt;_ast.Module object at 0x1a0fede828&gt;&lt;_ast.FunctionDef object at 0x1a0fede898&gt;&lt;_ast.Assign object at 0x1a0fedea58&gt;3.2.2 심벌 테이블  심벌 테이블(symbol table): 변수나 함수의 이름과 그 속성에 대해 기술해 놓은 테이블import symtablesym = symtable.symtable(s, "test.py", "exec") # symtable로 테이블을 받아오고print(sym.get_name()) # 이름을 확인 ('top': 이 테이블이 글로벌 테이블이라는 의미)print(sym.get_symbols()) # 현재 영역에 있는 심벌을 확인top[&lt;symbol 'func'&gt;, &lt;symbol 'a'&gt;, &lt;symbol 'b'&gt;, &lt;symbol 'c'&gt;, &lt;symbol 'print'&gt;]# 함수 func의 심벌테이블print(sym.get_children()) # 글로벌 심벌 테이블 내에 다른 심벌 테이블이 있는지 확인func_sym = sym.get_children()[0] # 테이블을 받아옴print(func_sym.get_name()) # 심벌테이블의 이름은 funcprint(func_sym.get_symbols()) # 심벌을 얻어오면 인자 a와 b를 볼 수 있음[&lt;Function SymbolTable for func in test.py&gt;]func[&lt;symbol 'a'&gt;, &lt;symbol 'b'&gt;]3.2.3. 바이트 코드와 PVM# 바이트 코드를 생성import disg = dis.get_instructions(s) # 바이트 코드를 제공하는 제너레이터 생성for inst in g:    print(inst.opname.ljust(20), end= " ") # 바이트 코드 이름    print(inst.argval) # 인자 값 출력, 바이트 코드 인스트럭션 확인LOAD_CONST           &lt;code object func at 0x1a0fb57030, file "&lt;disassembly&gt;", line 1&gt;LOAD_CONST           funcMAKE_FUNCTION        0STORE_NAME           funcLOAD_CONST           10STORE_NAME           aLOAD_CONST           20STORE_NAME           bLOAD_NAME            funcLOAD_NAME            aLOAD_NAME            bCALL_FUNCTION        2STORE_NAME           cLOAD_NAME            printLOAD_NAME            cCALL_FUNCTION        1POP_TOP              NoneLOAD_CONST           NoneRETURN_VALUE         None추가: PVM - CPython 소스 코드 중 ceval.c에 있는 PVM 일부 코드PyObject* _Py_HOT_FUNCTION_PyEval_EvalFrameDefault(PyFrameObject *f, int throwflag){--------------- 중략 ---------------main_loop:    for (;;) { /* 무한 루프 (PVM)        assert(stack_pointer &gt;= f-&gt;f_valuestack); /* else underflow */        assert(STACK_LEVEL() &lt;= co-&gt;co_stacksize);  /* else overflow */        assert(!PyErr_Occurred());--------------- 중략 ---------------switch (opcode) { /* 실제 바이트 코드를 분석해서 실행        /* BEWARE!           It is essential that any operation that fails must goto error           and that all operation that succeed call [FAST_]DISPATCH() ! */        TARGET(NOP)            FAST_DISPATCH();        TARGET(LOAD_FAST) {            PyObject *value = GETLOCAL(oparg);            if (value == NULL) {                format_exc_check_arg(PyExc_UnboundLocalError,                                     UNBOUNDLOCAL_ERROR_MSG,                                     PyTuple_GetItem(co-&gt;co_varnames, oparg));                goto error;            }            Py_INCREF(value);            PUSH(value);            FAST_DISPATCH();        }        PREDICTED(LOAD_CONST);        TARGET(LOAD_CONST) {            PyObject *value = GETITEM(consts, oparg);            Py_INCREF(value);            PUSH(value);            FAST_DISPATCH();        }        PREDICTED(STORE_FAST);        TARGET(STORE_FAST) {            PyObject *value = POP();            SETLOCAL(oparg, value);            FAST_DISPATCH();        }        --------------- 후략 ---------------]]></content>
      <categories>
        
          <category> python </category>
        
      </categories>
      <tags>
        
          <tag> python </tag>
        
          <tag> computer science </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[동적 기본 인수를 지정하기 위한 None과 docstring]]></title>
      <url>/python/2018/03/16/%EB%8F%99%EC%A0%81-%EA%B8%B0%EB%B3%B8-%EC%9D%B8%EC%88%98%EB%A5%BC-%EC%A7%80%EC%A0%95%ED%95%98%EA%B8%B0-%EC%9C%84%ED%95%9C-None%EA%B3%BC-docstring/</url>
      <content type="text"><![CDATA[  출처: 파이썬 코딩의 기술 (브렛 슬라킨, 길벗)이벤트 발생 시각을 포함해 로깅 메시지를 출력하려고 다음 함수를 작성하고 실행했다.# print time and message 1from datetime import datetimeimport timedef log(message, when=datetime.now()):    print("{}: {}".format(when, message))    log("Hi there!")time.sleep(0.1)log("Hi again!")2018-03-04 23:33:53.884102: Hi there!2018-03-04 23:33:53.884102: Hi again!이벤트 발생 시각을 포함해 로깅 메시지를 출력하려고 시도했지만 datetime.now는 함수를 정의할 때 딱 한 번만 실행되므로 타임스탬프가 동일하게 출력됨. 모듈이 로드된 후에는 기본 인수인 datetime.now는 다시 평가되지 않는다. 이를 목적대로 실행하기 위해서는 기본값을 None으로 설정하고 docstring으로 실제 동작을 문서화하는게 관례이다.다시 함수를 작성하고 실행하면 의도했던대로 잘 동작한다.# print time and message 2def log(message, when=None):    """Log message with a timestamp        Args:        message: Message to print.        when: datetime of when the message occurred.            Defaults to the present time.    """    when = datetime.now() if when is None else when    print("{}: {}".format(when, message))    log("Hi there!")time.sleep(0.1)log("Hi again!")2018-03-04 23:39:00.842097: Hi there!2018-03-04 23:39:00.945287: Hi again!핵심 정리  기본 인수는 모듈 로드 시점에 함수 정의 과정에서 딱 한 번만 평가됨. 그래서 동적 값에는 이상하게 동작하는 원인이 되기도 함  값이 동적인 키워드 인수에는 기본값으로 None을 사용하고, docstring을 통해 실제 동작을 문서화비슷한 예로, JSON 포맷의 데이터를 불러와 딕셔너리 형태로 생성하는 코드를 작성하면 다음과 같다.# JSON 데이터로 인코드된 값을 로드# 데이터 디코딩이 실패하면 기본으로 빈 딕셔너리를 리턴def decode(data, default={}):    try:        return json.loads(data)    except ValueError:        return default# 하나를 수정하면 다른 하나도 수정됨foo = decode('bad data')foo['stuff'] = 5bar = decode('also bad')bar['meep'] = 1print("Foo: ", foo)print("Bar: ", bar)Foo:  {'stuff': 5, 'meep': 1}Bar:  {'stuff': 5, 'meep': 1}이 함수 역시 의도했던대로 동작하게 하려면 None을 이용한다.def decode(data, default=None):    """Load JSON data from a string.        Args:        data: JSON data to decode.        default: Value to return if decoding fails.            Defaults to an empty dictionary.    """    if default is None:        default = {}        try:        return json.loads(data)    except ValueError:        return defaultfoo = decode('bad data')foo['stuff'] = 5bar = decode('also bad')bar['meep'] = 1print("Foo: ", foo)print("Bar: ", bar)Foo:  {'stuff': 5}Bar:  {'meep': 1}None을 이용하면 의도했던대로 잘 동작하는 것을 확인할 수 있다.]]></content>
      <categories>
        
          <category> python </category>
        
      </categories>
      <tags>
        
          <tag> python </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
</search>
