---
title: '[스파크] 01. 스파크 기본 개념'
description: 스파크를 구성하는 기본 개념에 대해 정리합니다.
date: 2019-07-05 10:00:00
categories:
- Data Engineering
- Spark
tags:
- data engineering
- spark
---

> 참고: 스파크2 프로그래밍 (위키북스, 백성민)
>



## RDD, Dataset, DataFrame

스파크는 스파크 프로그램 내에서 데이터를 표현하고 처리하기 위해 프로그래밍 모델을 제공하는데, RDD, 데이터셋, 데이터프레임이 그것입니다. RDD는 스파크의 기본 모델이고 데이터셋과 데이터프레임은 추후 새롭게 도입된 모델입니다.



### RDD

#### RDD 기본 개념

RDD (Resilient Distributed Dataset)는 스파크 내부에 존재하는 분산 데이터에 대한 모델로, 단순히 값으로 표현되는 데이터만 가리키는 것이 아니라 데이터를 다루는 방법까지 포함하는 일종의 클래스 같은 개념입니다.

[스파크 공식 홈페이지](https://spark.apache.org/docs/latest/rdd-programming-guide.html#resilient-distributed-datasets-rdds)에서는 RDD를 아래와 같이 정의하고 있습니다. 이에 따르면 RDD는 분산 방식으로 저장된 데이터 요소들의 집합을 의미하며, 병렬 처리가 가능하고 장애가 발생할 경우에도 스스로 복구할 수 있는 내성을 가지고 있습니다.

> Spark revolves around the concept of a *resilient distributed dataset* (RDD), which is a fault-tolerant collection of elements that can be operated on in parallel.

다르게 표현하면 RDD는 스파크에서 정의한 분산 데이터 모델로서 병렬 처리가 가능한 요소로 구성되고 데이터를 처리하는 과정에서 일시적인 문제가 발생하더라도 스스로 에러를 복구할 수 있는 능력을 가진 데이터 모델이라고 할 수 있습니다. 스스로 에러를 복구한다는 부분을 통해 RDD가 값에 해당하는 데이터 뿐만 아니라 데이터 처리 방법 및 실제적인 데이터 처리 능력도 함께 가지고 있다는 점도 짐작할 수 있습니다.

하나의 RDD에 속한 요소들은 파티션이라고 하는 단위로 나뉠 수 있는데 스파크는 작업을 수행할 때 이 파티션 단위로 나눠 병렬로 처리를 수행합니다. 하나의 RDD가 여러 파티션으로 나뉘어 다수의 서버에서 처리하다 보면 작업 도중 일부 파티션 처리에 장애가 발생해서 파티션 처리 결과가 유실될 수 있습니다. 스파크는 이렇게 손상된 RDD를 원 상태로 복원하기 위해 RDD의 생성 과정을 기록해 뒀다가 다시 복구하는 기능을 가지고 있습니다. 엄밀히는 RDD에 포함된 데이터를 저장하는 것이 아니고 RDD를 생성하는 데 사용했던 작업 내용을 저장하고 있습니다. 그래서 문제가 발생하면 문제가 발생한 RDD를 생성했던 작업만 다시 수행해서 복구합니다. 단 이러한 복구를 위해서는 한번 생성된 RDD는 바뀌지 않아야 한다는 조건이 필요합니다. RDD가 생성된 이후에 변경이 가능하면 RDD를 생성하는데 필요한 작업 뿐만 아니라 RDD를 변경시킨 모든 작업에 대한 기록을 저장해야 하기 때문입니다.



#### RDD 생성

RDD는 크게 세 가지 방법으로 생성할 수 있습니다. (공식 홈페이지에서는 두 개의 방법을 소개하고 있습니다.) 

첫 번째 방법은 기존 프로그램의 메모리에 생성된 데이터를 이용하는 것입니다. (`sc.parallelize()`) 

[python]

```python
rdd = sc.paralleize(["a", "b", "c", "d", "e"])
```

[scala]

```scala
val rdd = sc.paralleize(["a", "b", "c", "d", "e"])
```



두 번째 방법은 로컬 시스템이나 HDFS 같은 외부 저장소에 저장된 데이터를 읽어서 생성하는 방법입니다. 이 때 `HADOOP_HOME` 같은 환경 변수가 등록되어 있을 경우 파일시스템에 대한 스킴 정보 없이 `/path1/path2/` 로만 경로를 지정할 경우 HDFS 시스템 경로로 인식되므로 주의해야 합니다.

[python]

```python
rdd = sc.textFile("PATH")
```

[scala]

```scala
val rdd = sc.textFile("PATH")
```



세 번째 방법은 기존에 생성한 RDD로 또 다른 RDD를 생성하는 방법입니다. 이 경우 별도의 생성 함수가 있는 것은 아니고, 기존 RDD의 모든 요소에 어떤 연산을 적용하면 (한번 만들어지면 수정되지 않는다는 RDD의 특성 때문에) 새로운 RDD가 만들어지는 것을 의미합니다.

[python]

```python
rdd1 = rdd.map(lambda s: s.upper())
```

[scala]

```scala
val rdd1 = rdd.map(_.toUpperCase())
```



#### Transformation/Action

RDD를 생성하고 나면 RDD가 제공하는 다양한 연산을 이용해 데이터를 처리하면 됩니다. 이 때 RDD에서 제공하는 연산은 크게 트랜스포메이션<sup>Transformation</sup> 과 액션<sup>Action</sup> 으로 분류할 수 있습니다.

트랜스포메이션은 RDD에 어떤 변형을 가해 새로운 RDD를 생성하는 연산으로, 기존 RDD는 변하지 않은 채 변형된 값을 가진 새로운 RDD를 생성하는 연산입니다. 트렌스포메이션의 중요한 특징으로는 연산이 호출되는 시점에 바로 실행되는 것이 아니라 데이터 변환을 어떻게 수행할지에 관한 정보만 누적해서 가지고 있다가 액션에 해당하는 연산이 호출될 때에 한꺼번에 실행된다는 점입니다.

액션 연산은 그 연산의 결과로 RDD가 아닌 다른 값을 반환하거나 아예 반환하지 않는 연산을 뜻합니다. RDD의 크기를 계산해서 `Long` 값을 반환하거나 RDD를 출력 또는 외부에 저장하는 등의 동작이 여기에 해당합니다.

