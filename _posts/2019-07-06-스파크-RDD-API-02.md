---
title: '[스파크] 02. RDD API - Transformation (2)'
description: 스파크 RDD의 기본 API를 정리합니다.
date: 2019-07-06 16:00:00
categories:
- Data Engineering
- Spark
tags:
- data engineering
- spark
---

> 참고: 스파크2 프로그래밍 (위키북스, 백성민)
>



# RDD API

## Transformation

### zip

`zip()` 연산은 두 개의 서로 다른 RDD를 각 요소의 인덱스에 따라 하나의 (키, 값) 쌍으로 묶어줍니다. 즉 첫 번째 RDD의 n번 째 요소를 키, 두 번째 RDD의 n번 째 요소를 값으로 가지는 쌍을 생성합니다. 이 때 두 RDD는 같은 개수의 파티션을 가지고 있고 각 파티션에 속하는 요소의 수는 동일하다고 가정합니다. 그래서 서로 크기가 다른 RDD 간에는 `zip()` 을 사용할 수 없습니다.

```scala
def doZip(sc: SparkContext): Unit = {
  val rdd1 = sc.parallelize(List("a", "b", "c"))
  val rdd2 = sc.parallelize(List(1, 2, 3))
  val result = rdd1.zip(rdd2)
  println(result.collect.mkString(", "))
}

>>>
(a,1), (b,2), (c,3)
```



### zipPartitions

`zipPartitions()` 는 파티션 단위로 `zip()` 을 수행하고 특정 함수를 적용한 뒤 그 결과로 구성된 새로운 RDD를 구성하는 RDD입니다. `zipPartitions()` 는 `zip()` 과 몇 가지 차이점이 있습니다.

먼저 `zipPartitions()` 는 두 RDD의 파티션 개수만 동일해도 됩니다. 두 번째로 `zip()` 에서는 최대 1개의 RDD만 인자로 넘길 수 있지만 `zipPartitions()` 는 최대 4개의 RDD를 사용 가능합니다.마지막으로 `zip()` 은 두 RDD 요소의 쌍을 만드는데 그치지만 `zipPartitions()` 는 병합에 사용할 함수를 인자로 전달받아 사용할 수 있습니다.

```scala
def doZipPartitions(sc: SparkContext): Unit = {
  val rdd1 = sc.parallelize(List("a", "b", "c"), 3)
  val rdd2 = sc.parallelize(List(1, 2), 3)
  val result = rdd1.zipPartitions(rdd2) {
    (it1, it2) =>
    val result = new ListBuffer[String]
    while (it1.hasNext) {
      if (it2.hasNext) result += (it1.next() + it2.next()) else result += it1.next()
    }
    result.iterator
  }
  println(result.collect.mkString(", "))
}

>>>
a, b1, c2
```

위의 예제에서 `rdd2` 의 파티션 중 하나는 빈 파티션이기 때문에 결괏값 중 하나는 `a` 라는 문자 하나만 포함하고 있습니다.

> [ListBuffer](https://www.scala-lang.org/api/current/scala/collection/mutable/ListBuffer.html)는 List로 구현한 `Buffer` 로, 앞 뒤로 원소를 추가하는데에 상수 시간이 걸립니다. 



### groupBy

`groupBy()` 는 RDD의 요소를 일정 기준에 따라 여러 그룹으로 나누고 이 그룹으로 구성된 새로운 RDD를 생성합니다. 각 그룹은 키와 그 키에 속한 요소의 시퀀스로 구성되고 메서드의 인자로 전달되는 함수가 각 그룹의 키를 결정하는 역할을 합니다.

```scala
def doGroupBy(sc: SparkContext): Unit = {
  val rdd = sc.parallelize(1 to 10)
  val result = rdd.groupBy {
    case i: Int if i % 2 == 0 => "even"
    case _ => "odd"
  }
  result.collect.foreach {
    v => println(s"${v._1}, [${v._2.mkString(", ")}]")
  }
}

>>>
even, [2, 4, 6, 8, 10]
odd, [1, 3, 5, 7, 9]
```



### groupByKey

 `groupBy()` 가 키를 생성하는 작업과 분류하는 작업을 동시에 수행한다면, `groupByKey()` 는 이미 키/값 쌍으로 이루어진 RDD에 대해 사용 가능합니다. 키를 기준으로 같은 키를 가진 요소들로 그룹을 만들고 이 그룹들로 구성된 새로운 RDD를 생성합니다.

```scala
def doGroupByKey(sc: SparkContext): Unit = {
  val rdd = sc.parallelize(List("a", "b", "c", "b", "c")).map((_, 1))
  val result = rdd.groupByKey
  result.collect.foreach{
    v => println(s"${v._1}, [${v._2.mkString(", ")}]")
  }
}

>>>
a, [1]
b, [1, 1]
c, [1, 1]
```



### cogroup

`cogroup()` 은 RDD의 구성요소가 키/값 쌍으로 구성된 경우에만 사용할 수 있습니다. 여러 RDD에서 같은 키를 갖는 값 요소를 찾아 키와 그 키에 속하는 요소의 시퀀스로 구성된 튜플을 만들고, 그 튜플들로 구성된 새로운 RDD를 만듭니다.

```scala
def doCoGroup(sc: SparkContext): Unit = {
  val rdd1 = sc.parallelize(List(("k1", "v1"), ("k2", "v2"), ("k1", "v3")))
  val rdd2 = sc.parallelize(List(("k1", "v4")))
  val result = rdd1.cogroup(rdd2)
  result.collect.foreach {
    case (k, (v_1, v_2)) => {
      println(s"($k, [${v_1.mkString(", ")}], [${v_2.mkString(", ")}])")
    }
  }
}

>>>
(k1, [v1, v3], [v4])
(k2, [v2], [])

```

위 예제에서 `rdd1`, `rdd2` 모두 키/값 쌍으로 구성되어 있습니다. 두 RDD의 모든 키는 `k1` 과 `k2` 이고 `k1` 은 `rdd1` 과 `rdd2` 를 모두 가지고 있습니다. `result` 또한 두 키의 개수와 동일하게 두 개의 요소를 가지고 있습니다. 각 요소는 튜플로 구성되는데 첫 번째 요소는 키이고 두 번째 요소는 해당 키에 속하는 시퀀스로 구성된 또 다른 튜플입니다. 이 튜플에서 첫 번째 시퀀스는 `rdd1` 에 속했던 요소들이고, 두 번째 시퀀스는 `rdd2` 에 속했던 요소들입니다. 즉 `Tuple(key, Tuple(rdd1 요소들의 집합, rdd2 요소들의 집합))` 처럼 나타낼 수 있습니다. 위에서 `rdd2.cogroup(rdd1)` 으로 바꾸면 `Tuple(key, Tuple(rdd2 요소들의 집합, rdd1 요소들의 집합))` 로 바뀝니다.



### distinct

`distinct()` 는 RDD에서 중복을 제거한 새로운 RDD를 생성합니다.

```scala
def doDistinct(sc: SparkContext): Unit = {
  val rdd = sc.parallelize(List(1, 2, 3, 1, 2, 3, 1, 2, 3))
  val result = rdd.distinct()
  println(result.collect.mkString(", "))
}

>>>
1, 2, 3
```



### cartesian

`cartesian()` 은 두 RDD의 요소의 카테시안곱을 구한 요소로 새로운 RDD를 생성합니다.

```scala
def doCartesian(sc: SparkContext): Unit = {
  val rdd1 = sc.parallelize(List(1, 2, 3))
  val rdd2 = sc.parallelize(List("a", "b", "c"))
  val result = rdd1.cartesian(rdd2)
  println(result.collect.mkString(", "))
}

>>>
(1,a), (1,b), (1,c), (2,a), (2,b), (2,c), (3,a), (3,b), (3,c)
```



### subtract

`subtract()` 는 인자로 전달하는 RDD에는 속하지 않는 요소로 구성된 RDD를 생성합니다.

```scala
def doSubtract(sc: SparkContext): Unit = {
  val rdd1 = sc.parallelize(List("a", "b", "c", "d", "e"))
  val rdd2 = sc.parallelize(List("d", "e"))
  val result = rdd1.subtract(rdd2)
  println(result.collect.mkString(", "))
}

>>>
a, b, c

```



### union

두 RDD 요소의 합집합으로 새로운 RDD를 생성합니다.

```scala
def doUnion(sc: SparkContext): Unit = {
  val rdd1 = sc.parallelize(List("a", "b", "c"))
  val rdd2 = sc.parallelize(List("d", "e", "f"))
  val result = rdd1.union(rdd2)
  println(result.collect.mkString(", "))
}

>>>
a, b, c, d, e, f
```



### intersection

두 RDD에 공통으로 속하는 요소로 새로운 RDD를 생성합니다. 결과로 생성되는 RDD에는 중복 요소가 없습니다.

```scala
def doIntersection(sc: SparkContext): Unit = {
  val rdd1 = sc.parallelize(List("a", "a", "b", "c"))
  val rdd2 = sc.parallelize(List("a", "a", "c", "c"))
  val result = rdd1.intersection(rdd2)
  println(result.collect.mkString(", "))
}

>>>
a, c
```



### join

`join()` 은 RDD의 구성요소가 키/값으로 구성된 경우에 사용할 수 있습니다. 두 RDD에서 서로 같은 키를 가지고 있는 요소로 그룹을 만들고 이 결과로 새로운 RDD를 구성합니다.

결과 RDD는 `Tuple(key, Tuple(첫 번째 RDD의 요소, 두 번째 RDD의 요소))` 의 튜플 타입 요소를 가집니다.

```scala
def doJoin(sc: SparkContext): Unit = {
  val rdd1 = sc.parallelize(List("a", "b", "c", "d", "e")).map((_, 1))
  val rdd2 = sc.parallelize(List("b", "c")).map((_, 2))
  val result = rdd1.join(rdd2)
  println(result.collect.mkString("\n"))
}

>>>
(b,(1,2))
(c,(1,2))
```



### leftOuterJoin, rightOuterJoin

왼쪽/오른쪽 외부 조인을 수행합니다.

```scala
def doOuterJoin(sc: SparkContext): Unit = {
  val rdd1 = sc.parallelize(List("a", "b", "c")).map((_, 1))
  val rdd2 = sc.parallelize(List("b", "c")).map((_, 2))
  val result1 = rdd1.leftOuterJoin(rdd2)
  val result2 = rdd1.rightOuterJoin(rdd2)
  println("Left: " + result1.collect.mkString("\t"))
  println("Right: " + result2.collect.mkString("\t"))
}

>>>
Left: (a,(1,None))	(b,(1,Some(2)))	(c,(1,Some(2)))
Right: (b,(Some(1),2))	(c,(Some(1),2))
```

조인 결과를 표시할 때는 값이 존재하지 않는 경우를 고려해 Option 타비을 이용합니다.



### substractByKey

`rdd1.substractByKey(rdd2)` 는 rdd1의 요소 중에서 rdd2에 같은 키가 존재하는 요소를 제외한 나머지로 구성된 새로운 RDD를 리턴합니다.

```scala
def doSubtractByKey(sc: SparkContext): Unit = {
  val rdd1 = sc.parallelize(List("a", "b")).map((_, 1))
  val rdd2 = sc.parallelize(List("b")).map((_, 2))
  val result = rdd1.subtractByKey(rdd2)
  println(result.collect.mkString("\n"))
}

>>>
(a,1)
```

