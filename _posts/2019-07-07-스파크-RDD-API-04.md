---
title: '[스파크] 02. RDD API - Action'
description: 스파크 RDD의 기본 API를 정리합니다.
date: 2019-07-08 18:00:00
categories:
- Data Engineering
- Spark
tags:
- data engineering
- spark
---

> 참고: 스파크2 프로그래밍 (위키북스, 백성민)
>



# RDD API

## Action

Action은 RDD 메서드 중에서 결괏값이 RDD가 아닌 메서드들을 통칭해서 부르는 용어입니다. 결괏값이 RDD인 메서드는 transformation으로 분류되는데 이것들은 lazy evaluation 방식을 채택하고 있습니다. 따라서 action으로 분류되는 메서드가 호출되어야 transformation 메서드가 비로소 실행됩니다. `map()` 등의 메서드를 100번 호출한다 하더라고 `count()` 같은 action 메서드가 호출되는 시점에 100개의 연산이 순차적으로 실행되는 것입니다.

이 때 action 메서드를 여러 번 호출하면 transformation 도 여러 번 실행됩니다. 따라서 이 경우 성능 개선을 위해 캐시를 적절히 사용해야 하고 반복 수행에 따른 오류가 발생하지 않게 코드를 작성해야 합니다.



### first

RDD 요소 가운데 첫 번째 요소 하나를 리턴합니다.

```scala
def doFirst(sc: SparkContext): Unit = {
  val rdd = sc.parallelize(List(5, 4, 1))
  val result = rdd.first
  println(result)
}

>>>
5
```



### take

RDD의 첫 n개의 요소를 리턴합니다.

```scala
def doTake(sc: SparkContext): Unit = {
  val rdd = sc.parallelize(1 to 20, 5)
  val result = rdd.take(5)
  println(result.mkString(", "))
}

>>>
1, 2, 3, 4, 5
```



### takeSample

RDD 요소 가운데 지정된 크기의 샘플을 추출합니다. `sample()` 과 다른 점은 샘플의 크기를 지정할 수 있다는 점과 리턴 타입이 배열이나 리스트 같은 컬렉션 타입이라는 점입니다.

```scala
def doTakeSample(sc: SparkContext): Unit = {
  val rdd = sc.parallelize(1 to 100)
  val result = rdd.takeSample(false, 20)
  println(result)
}

>>>
20
```



### countByValue

RDD에 속하는 각 값들의 횟수를 구해서 맵 형태로 리턴합니다.

```scala
def doCountByValue(sc: SparkContext): Unit = {
  val rdd = sc.parallelize(List(1, 1, 2, 3, 3))
  val result = rdd.countByValue
  println(result)
}

>>>
Map(1 -> 2, 2 -> 1, 3 -> 2)
```



### reduce

`reduce()` 는 RDD에 포함된 임의의 값 두 개를 하나로 합치는 함수를 이용해 RDD에 포함된 모든 요소를 하나의 값으로 병합한 뒤 그 결괏값을 리턴하는 메서드입니다.

주의할 점은 스파크 애플리케이션이 클러스터 환경에서 동작하는 분산 프로그램이기 때문에 실제 병합은 첫 번째부터 마지막 요소까지 순차적으로 이루어지는 것이 아니라 각 서버에 흩어져. 있는 파티션 단위로 나눠져서 처리된다는 것입니다. 따라서 메서드에 적용되는 병합 연산은 RDD의 모든 요소에 대해 교환법칙과 결합법칙이 성립해야 합니다.

```scala
def doReduce(sc: SparkContext): Unit = {
  val rdd = sc.parallelize(1 to 10, 3)
  val result = rdd.reduce(_ + _)
  println(result)
}

>>>
55
```



### fold

`reduce()` 와 비슷하지만 병합 연산에 초깃값을 지정할 수 있습니다.

```scala
def doFold(sc: SparkContext): Unit = {
  val rdd = sc.parallelize(1 to 10, 3)
  val result = rdd.fold(0)(_ + _)
  println(result)
}

>>>
55
```



`reduce()` 와 `fold()` 는 비슷하지만 작동 방식이 다른데, `reduce()` 는 RDD에 포함된 요소만으로 병합을 수행하기 때문에 빈 파티션에 대해서는 처리를 하지 않지만 `fold()` 는 초깃값 때문에 빈 파티션에 대해서도 최소 한번의 연산이 수행됩니다. 따라서 예상치 못한 문제를 방지하기 위해 주의해야 합니다.

```scala
def doReduceVsFold(sc: SparkContext): Unit = {
  case class Prod(var price: Int) {
    var cnt = 1
  }
  // Empty partitions exist
  val rdd = sc.parallelize(List(Prod(300), Prod(200), Prod(100)), 10)

  // reduce
  val r1 = rdd.reduce((p1, p2) => {
    p1.price += p2.price
    p1.cnt += 1
    p1
  })
  println(s"Reduce: (${r1.price}, ${r1.cnt})")

  // fold
  val r2 = rdd.fold(Prod(0))((p1, p2) => {
    p1.price += p2.price
    p1.cnt += 1
    p1
  })
  println(s"Fold: (${r2.price}, ${r2.cnt})")
}

>>>
Reduce: (600, 3)
Fold: (600, 11)
```

위의 예에서 `reduce()` 의 결과는 제대로 나온 반면 `fold()` 는 빈 파티션에 대해서도 연산이 수행되어서 상품 개수가 11개로 잘못 집계된 것을 알 수 있습니다.



### aggregate

앞서 살펴본 `reduce()` 나 `fold()` 는 입력과 출력 타입이 동일해야 한다는 제약을 가집니다. 반면 `aggregate()` 에는 그러한 제약이 없습니다.

```scala
def aggregate[U](zeroValue : U)(seqOp : scala.Function2[U, T, U], combOp : scala.Function2[U, U, U])(implicit evidence$30 : scala.reflect.ClassTag[U]) : U
```

스파크 문서에 따르면 `aggregate()` 메서드는 두 단계에 걸쳐 병합을 처리합니다. 첫 단계에서는 파티션 단위로 병합을 수행하는데 이 때 `seqOp` 함수가 사용됩니다. 다음 단계에서는 `combOp` 함수를 이용해 파티션 단위 병합 결과로 최종 결과를 생성합니다.

```scala
def doAggregate(sc: SparkContext): Unit = {
  val rdd = sc.parallelize(List(100, 80, 75, 90, 95), 3)
  val zeroValue = Record(0, 0)
  val seqOp = (r: Record, v: Int) => r.add(v)
  val combOp = (r1: Record, r2: Record) => r1 add r2
  val result1 = rdd.aggregate(zeroValue)(seqOp, combOp)
  println(result1.amount/result1.number)
}

>>>
88
```



### sum

RDD의 요소가 double, Long 등 숫자 타입일 경우에만 사용 가능하며 전체 요소의 합을 구합니다.

```scala
def doSum(sc: SparkContext): Unit = {
  val rdd = sc.parallelize(1 to 10)
  val result = rdd.sum
  println(result)
}

>>>
55.0
```



### foreach & foreachPartition

`foreach()` 는 RDD의 모든 요소에 특정 함수를 적용하는 메서드입니다. `foreachPartition()` 은 파티션 단위로 함수를 적용합니다. 주의할 점은 인자로 전달받은 함수가 드라이버 프로그램 (main 프로그램)이 동작하는 서버가 아닌 클러서터 각 개별 노드에서 실행된다는 점입니다. 따라서 실제로는 각 노드에서 실행돼도 의미 있는 작업을 수행해야 합니다.

```scala
def doForEachForEachPartition(sc: SparkContext): Unit = {
  val rdd = sc.parallelize(1 to 10, 3)
  rdd.foreach {v =>
    println(s"Value Side Effect: ${v}")
  }
  rdd.foreachPartition {values =>
    println("Partition Side Effect!!")
    for (v <- values) println(s"Value Side Effect: ${v}")
  }
}

>>>
Value Side Effect: 1
Value Side Effect: 4
Value Side Effect: 7
Value Side Effect: 5
Value Side Effect: 2
Value Side Effect: 6
Value Side Effect: 8
Value Side Effect: 3
Value Side Effect: 9
Value Side Effect: 10
Partition Side Effect!!
Value Side Effect: 7
Value Side Effect: 8
Value Side Effect: 9
Value Side Effect: 10
Partition Side Effect!!
Value Side Effect: 4
Value Side Effect: 5
Value Side Effect: 6
Partition Side Effect!!
Value Side Effect: 1
Value Side Effect: 2
Value Side Effect: 3
```



### toDebugString

디버깅을 위한 메서드입니다. RDD의 파티션 개수나 의존성 정보등 세부 정보를 알고 싶을 때 사용할 수 있습니다.

```scala
def doDebugToString(sc: SparkContext): Unit = {
  val rdd = sc.parallelize(1 to 100, 10).map(_ * 2).persist.map(_ + 1).coalesce(2)
  println(rdd.toDebugString)
}

>>>
(2) CoalescedRDD[3] at coalesce at RDDOPSample.scala:422 []
 |  MapPartitionsRDD[2] at map at RDDOPSample.scala:422 []
 |  MapPartitionsRDD[1] at map at RDDOPSample.scala:422 []
 |  ParallelCollectionRDD[0] at parallelize at RDDOPSample.scala:422 []
```

(2) 는 최종 파티션 개수를 의미합니다. 이후의 내용은 최초 RDD 생성 후 두 번의 map 연산이 있었음을 나타냅니다.



### cache, persist, unpersist

RDD는 action이 수행될 때마다 관련 transformation 연산을 수행합니다. 이 때 기존에 사용했던 데이터가 메모리에 남아 있다면 그 데이터를 사용하지만 그렇지 않다면 RDD 생성 히스토리를 이용해 데이터를 복구 (재생성)합니다. 따라서 다수의 action 연산을 통해 반복적으로 사용되는 RDD의 경우 해당 데이터를 메모리 등에 저장해 두는 것이 매번 새로운 RDD를 만들어 내는 것보다 유리할 것입니다.

`cache()` 와 `persist()` 는 첫 action을 실행한 후 RDD 정보를 메모리 또는 디스크 등에 저장해서 다음 action을 수행할 때 불필요한 재생성을 거치지 않고 원하는 작업을 즉시 실행할 수 있게 하는 메서드입니다. `cache()` 는 RDD의 데이터를 메모리에 저장하라는 의미이며 메모리가 부족하다면 부족한 만큼 저장을 수행하지 않습니다. `persist()` 는 `StorageLevel` 이라는 옵션으로 저장 위치와 저장 방식을 상세히 지정할 수 있습니다.

`unpersist()` 는 데이터가 더 이상 필요없을 때 캐시 설정을 취소하는 데 사용합니다.

```scala
def doCachePersistUnpersist(sc: SparkContext): Unit = {
  val rdd = sc.parallelize(1 to 100, 10)
  rdd.cache
  rdd.persist(StorageLevel.MEMORY_ONLY)
  rdd.unpersist()
}
```



### partitions

RDD의 파티션 정보가 담긴 배열을 리턴합니다. 배열의 요소는 `Partition` 타입이고 파티션의 인덱스 정보를 알려주는 `index()` 메서드를 포함하고 있습니다. 단순히 크기 정보를 알기 위한 목적이라면 `getNumPartitions()` 를 사용하는 것이 좀 더 편리합니다.

```scala
def doPartitions(sc: SparkContext): Unit = {
  val rdd = sc.parallelize(1 to 1000, 10)
  println(rdd.partitions.length)
  println(rdd.getNumPartitions)
}

>>>
10
10
```

