---
title: '[스파크] 02. RDD API - Transformation (3)'
description: 스파크 RDD의 기본 API를 정리합니다.
date: 2019-07-07 00:00:00
categories:
- Data Engineering
- Spark
tags:
- data engineering
- spark
---

> 참고: 스파크2 프로그래밍 (위키북스, 백성민)
>



# RDD API

## Transformation

### reduceByKey

같은 키를 가진 값들을 하나로 병합해 키/값 쌍으로 구성된 새로운 RDD를 생성합니다.

```scala
def doReduceByKey(sc: SparkContext): Unit = {
  val rdd = sc.parallelize(List("a", "b", "b")).map((_, 1))
  val result = rdd.reduceByKey(_ + _)
  println(result.collect.mkString(", "))
}

>>>
(a,1), (b,2)
```



### foldByKey

전반적인 동작은 `reduceByKey()` 와 비슷하지만 병합 연산의 초깃값을 인자로 전달할 수 있다는 점에서 차이가 있습니다. 단 초깃값은 연산 결과에 영향을 주지 않는 값이어야 합니다.

```scala
def doFoldByKey(sc: SparkContext): Unit = {
  val rdd = sc.parallelize(List("a", "b", "c")).map((_, 1))
  val result = rdd.foldByKey(0)(_ + _)
  println(result.collect.mkString(", "))
}

>>>
(a,1), (b,2)
```

위에서는 정수를 더하는 연산이므로 `0` 을 초깃값으로 사용했습니다.



### combineByKey

앞의 두 메서드와 비슷하지만 병합 과정에서 값의 타입이 바뀔 수 있습니다. 한편 인자로 지정된 3개의 함수의 의미는 다음과 같습니다.

- `createCombiner : scala.Function1[V, C]`: 값을 병합하기 위한 combiner를 생성합니다. combiner는 두 개의 값을 하나로 병합하는 객체입니다.
- `mergeValue : scala.Function2[C, V, C]`: 키에 대한 combiner가 이미 존재한다면 새로운 combiner를 만들지 ㅇ낳고 이 함수를 이용해 값을 기존 combiner에 병합합니다. 따라서 각 키별 combiner 내부에는 병합된 값을 누적되어 쌓입니다.
- `mergeCombiners : scala.Function2[C, C, C])`: `createCombiner()` 와 `mergeValue()` 는 파티션 단위로 수행됩니다. `mergeCombiners()` 는 병합이 끝난 combiner들끼리 다시 병합을 수행해 최종 combiner를 생성합니다.  

```scala
case class Record(var amount: Long, var number: Long = 1) {
  def map(v: Long) = Record(v)
  def add(amount: Long): Record = {
    add(map(amount))
  }
  def add(other: Record): Record = {
    this.number += other.number
    this.amount += other.amount
    this
  }
  // 평균을 계산합니다.
  override def toString: String = s"avg:${amount / number}"
}


// 과목별 평균을 계산합니다.
def doCombineByKey(sc: SparkContext): Unit = {
  val data = Seq(("Math", 100L), ("Eng", 80L), ("Math", 60L), ("Eng", 60L), ("Eng", 90L))
  val rdd = sc.parallelize(data)
  val createCombiner = (v: Long) => Record(v)
  val mergeValue = (c: Record, v: Long) => c.add(v)
  val mergeCombiners = (c1: Record, c2: Record) => c1.add(c2)
  val result = rdd.combineByKey(createCombiner, mergeValue, mergeCombiners)
  println(result.collect.mkString("\n"))
}

>>>
(Math,avg:80)
(Eng,avg:76)
```



### aggregateByKey

`combineByKey()` 의 특수한 경우로 병합을 시작할 초깃값을 생성하는 부분을 제외하면 `combineByKey()` 와 동일하게 동작합니다.

```scala
def aggregateByKey[U](zeroValue : U)(seqOp : scala.Function2[U, V, U], combOp : scala.Function2[U, U, U])(implicit evidence$3 : scala.reflect.ClassTag[U]) : org.apache.spark.rdd.RDD[scala.Tuple2[K, U]] = { /* compiled code */ }
```

메서드 설명을 보면 `zeroValue` 를 받는데 이 부분이 바로 초깃값을 가리킵니다. 또한 `combineByKey()` 는 병합을 위한 초깃값에 `createCombiner()` 라는 함수를 사용하고 `aggregateByKey()` 는 값을 사용한다는 점이 다릅니다.



```scala
def doAggregateByKey(sc: SparkContext): Unit = {
  val data = Seq(("Math", 100L), ("Eng", 80L), ("Math", 60L), ("Eng", 60L), ("Eng", 90L))
  val rdd = sc.parallelize(data)

  val zero = Record(0, 0)
  val mergeValue = (c: Record, v: Long) => c.add(v)
  val mergeCombiners = (c1: Record, c2: Record) => c1.add(c2)
  val result = rdd.aggregateByKey(zero)(mergeValue, mergeCombiners)
  println(result.collect.mkString("\n"))
}

>>>
(Math,avg:80)
(Eng,avg:76)
```



 ### pipe

`pipe()` 를 통해 외부 프로세스를 활용할 수 있습니다. 아래는 리눅스의 `cut` 을 이용해 첫 번째와 세 번째 숫자를 뽑아내는 예제입니다.

```scala
def doPipe(sc: SparkContext): Unit = {
  val rdd = sc.parallelize(List("1,2,3", "4,5,6", "7,8,9"))
  val result = rdd.pipe("cut -f 1,3 -d ,")
  println(result.collect.mkString(", "))
}

>>>
1,3, 4,6, 7,9
```



### coalesce & repartition

파티션 개수를 조정합니다. `coalesce()` 는 줄이는 것만 가능하며, `repartition()` 보다 성능이 상대적으로 좋습니다.

```scala
def doCoalesceRepartition(sc: SparkContext): Unit = {
  val rdd1 = sc.parallelize(1 to 100000, 10)
  val rdd2 = rdd1.coalesce(5)
  val rdd3 = rdd2.repartition(10)
  println(s"partitions size: ${rdd1.getNumPartitions}")
  println(s"partitions size: ${rdd2.getNumPartitions}")
  println(s"partitions size: ${rdd3.getNumPartitions}")
}

>>>
partitions size: 10
partitions size: 5
partitions size: 10
```



### repartitionAndSortWithinPartitions

`repartitionAndSortWithinPartitions()` 는 RDD를 구성하는 모든 데이터를 특정 기준에 따라 여러 개의 파티션으로 분리하고 각 파티션 단위로 정렬을 수행한 뒤 이 결과로 새로운 RDD를 생성하는 메서드입니다. 이 메서드를 사용하려면 데이터가 키/값 쌍으로 구성돼 있어야 하고 메서드를 실행할 때 각 데이터가 어떤 파티션에 속할지 결정하기 위한 파티셔너를 설정해야 합니다.

파티셔너는 각 데이터의 키를 이용해 데이터가 속할 파티션을 결정하는데 이 때 키를 이용한 정렬도 함께 수행합니다. 따라서 파티션과 정렬을 따로 하는 것보다 성능을 높힐 수 있습니다.

```scala
def doRepartitionAndSortWithinPartitions(sc: SparkContext): Unit = {
  val r = scala.util.Random
  val data = for (_ <- 1 to 10) yield (r.nextInt(100), "-")
  val rdd1 = sc.parallelize(data)
  val rdd2 = rdd1.repartitionAndSortWithinPartitions(new HashPartitioner(3))
  // 출력 결과가 정상적으로 보이지 않는 문제를 해결하기 위한 호출
  rdd2.count
  rdd2.foreachPartition(it => {
    println("==========")
    it.foreach(v => println(v))
  })
}

>>>
==========
(6,-)
(9,-)
(87,-)
==========
(28,-)
(28,-)
(58,-)
(64,-)
(91,-)
==========
(35,-)
(53,-)
```

파티셔너로 HashPartitioner를 설정하고 크기를 3으로 지정했습니다. 이에 따라 모든 데이터는 3으로 나눈 나머지 값에 따라 총 3개의 파티션으로 분리됩니다. 결과를 확인하면 키에 따라 (3으로 나눈 나머지가 0, 1, 2) 파티션이 나뉘고 파티션 내에서는 키를 따라 정렬되어 있는 것을 볼 수 있습니다.



### partitionBy

`partitionBy()` 는 RDD가 키/값으로 구성되어 있을 때 사용 가능한 파티션 변경 메서드로 `org.apache.spark.Partitioner` 클래스의 인스턴스를 인자로 전달해야 합니다. Partitioner는 각 요소의 키를 특정 파티션에 할당하는 역할을 합니다. 스파크에서 제공하는 기본적인 Partitioner에는 [HashPartitioner](https://spark.apache.org/docs/2.1.0/api/scala/index.html#org.apache.spark.HashPartitioner)와 [RangePartitioner](https://spark.apache.org/docs/2.1.0/api/scala/index.html#org.apache.spark.RangePartitioner)가 있습니다. 커스터마이징 시에는 Partitioner 클래스를 상속해서 구현한 뒤 인자로 전달하면 됩니다.

```scala
def doPartitionBy(sc: SparkContext): Unit = {
  val rdd1 = sc.parallelize(List("apple", "mouse", "monitor"), 5).map {a=> (a, a.length)}
  val rdd2 = rdd1.partitionBy(new HashPartitioner(3))
  println(s"rdd1:${rdd1.getNumPartitions}, rdd2:${rdd2.getNumPartitions}")
}

>>>
rdd1:5, rdd2:3
```

처음에 5개로 설정한 파티션 개수가 3개로 변경되었습니다.



### filter

`filter()` 는 RDD의 요소 중에서 원하는 요소만 걸러내는 메서드입니다.

```scala
def doFilter(sc: SparkContext): Unit = {
  val rdd = sc.parallelize(1 to 5)
  val result = rdd.filter(_ > 2)
  println(result.collect.mkString(", "))
}

>>>
3, 4, 5
```



###sortByKey

`sortByKey()` 는 키를 기준으로 요소를 정렬하는 메서드입니다. 정렬 후에 파티션 내부의 요소는 정렬 순서상 인접한 요소로 재구성됩니다.

```scala 
def doSortByKey(sc: SparkContext): Unit = {
  val rdd = sc.parallelize(List("q", "z", "a"))
  val result = rdd.map((_, 1)).sortByKey()
  println(result.collect.mkString(", "))
}

>>>
(a,1), (q,1), (z,1)
```



### keys & values

python `dict` 타입의 `keys()`, `values()` 와 같은 역할을 합니다.

```scala
def doKeysValues(sc: SparkContext): Unit = {
  val rdd = sc.parallelize(List(("k1", "v1"), ("k2", "v2"), ("k3", "v3")))
  println(rdd.keys.collect.mkString(", "))
  println(rdd.values.collect.mkString(", "))
}

>>>
k1, k2, k3
v1, v2, v3
```



### sample

샘플을 추출해 새로운 RDD를 생성합니다.

```scala
def doSample(sc: SparkContext): Unit = {
  val rdd = sc.parallelize(1 to 100)
  val result1 = rdd.sample(withReplacement = false, fraction = 0.5)
  val result2 = rdd.sample(withReplacement = true, fraction = 1.5)
  println(result1.take(5).mkString(", "))
  println(result2.take(5).mkString(", "))
}

>>>
1, 3, 5, 9, 16
3, 4, 4, 4, 8
```

`withReplacement` 는 복원/비복원 추출 여부를 결정합니다. `true` 면 복원, `false` 면 비복원 추출을 수행합니다. `fraction` 의 경우 복원, 비복원 추출일 경우 의미가 다릅니다. 복원 추출일 경우 각 요소의 평균 발생 횟수를 의미하며 반드시 0 이상이어야 합니다. 비복원 추출일 경우 각 요소가 샘플에 포함될 확률을 의미하며 0과 1사이의 값으로 지정할 수 있습니다.

