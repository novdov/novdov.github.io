---
title: 행렬의 미분 (Jacobian, Hessian)
date: 2018-05-18 14:19:49
description: ML에서 자주 사용되는 행렬 미분을 간단히 정리해봅니다.
categories:
- MachineLearning
tags:
- machine learning
---

## 스칼라를 벡터로 미분하는 경우

스칼라를 벡터로 미분하는 경우에는 결과를 열벡터로 표시한다. 이렇게 만들어진 벡터를 그래디언트 벡터(gradient vector)라고 부르며 $\nabla y$ 로 표기한다.

$$\nabla y =  \dfrac{\partial y}{\partial \mathbf{x}} = \begin{bmatrix} \dfrac{\partial y}{\partial x_1}\\\ \dfrac{\partial y}{\partial x_2}\\\ \vdots\\\ \dfrac{\partial y}{\partial x_N} \end{bmatrix}$$

2개의 독립 변수를 가지는 함수는 2차원 상에서 countour plot으로 나타낼 수 있다. 이 때 각 위치에서의 그래디언트 벡터를 화살표로 표현한 것을 quiver plot이라고 한다. quiver plot에서 그래디언트 벡터는 다음과 같은 특성을 가진다.

- 그래디언트 백터의 방향은 함수 곡면의 기울기가 가장 큰 방향, 즉 단위 길이당 함수 값이 가장 크게 증가하는 방향을 가리킨다.
- 그래디언트 벡터의 방향은 등고선의 방향과 직교한다.
- 그래디언트 벡터의 크기는 기울기를 의미한다. 즉 벡터의 크기가 클수록 함수 곡면의 기울기가 커진다.



## 행렬 미분 법칙

다변수 함수를 미분해 그래디언트를 구할 때는 다음 두 가지 법칙이 유용하게 쓰인다.

### 1. 선형 모형

선형 모형을 미분하면 가중치 벡터가 된다.

$$\dfrac{\partial \mathbf{w}^{T}\mathbf{x}}{\partial \mathbf{x}} = \dfrac{\partial \mathbf{x}^{T}\mathbf{w}}{\partial \mathbf{x}} = \mathbf{w}$$



### 2. 이차 형식 (Quadratic Form)

이차 형식을 미분하면 행렬과 벡터의 곱으로 나타난다.

$$\dfrac{\partial \mathbf{x}^{T}\mathbf{A}\mathbf{x}}{\partial \mathbf{x}} = (\mathbf{A} + \mathbf{A}^{T})\mathbf{x}$$



## 벡터를 스칼라로 미분하는 경우

벡터를 스칼라로 미분하는 경우에는 결과를 행벡터로 표시한다.

$$\dfrac{\partial \mathbf{y}}{\partial x} = \left[\dfrac{\partial y_1}{\partial x}\dfrac{\partial y_2}{\partial x}\cdots\dfrac{\partial y_M}{\partial x}\right]$$



## 벡터를 벡터로 미분하는 경우

함수의 종속 변수와 독립 변수가 모두 벡터(다차원) 데이터인 경우에는 독립 변수와 종속 변수 각각에 대해 모두 미분값이 존재한다. 따라서 도함수는 행렬 형태가 된다. 이렇게 만들어진 도함수 행렬을 **자코비안 행렬(Jacobian Matrix)** 라고 한다.

$$\mathbf J = \dfrac{d\mathbf y}{d\mathbf x} = \begin{bmatrix}\dfrac{\partial y_1}{\partial \mathbf x}^T \\\ \vdots \\\ \dfrac{\partial y_M}{\partial \mathbf x}^T \end{bmatrix} =\begin{bmatrix}\nabla y_1^T \\\ \nabla y_2^T \\\ \vdots \\\ \nabla y_M^T \end{bmatrix} =\begin{bmatrix}\dfrac{\partial y_1}{\partial x_1} & \cdots & \dfrac{\partial y_1}{\partial x_N}\\\ \vdots & \ddots & \vdots\\\ \dfrac{\partial y_M}{\partial x_1} & \cdots & \dfrac{\partial y_M}{\partial x_N} \end{bmatrix}$$



다변수 함수의 2차 도함수는 그래디언트 벡터를 독립 변수 벡터로 미분한 것이므로 다음과 같은 행렬로 나타낼 수 있다. 이러한 행렬을 **헤시안 행렬(Hessian Matrix)** 이라고 한다. 헤시안 행렬은 일반적으로 대칭 행렬이다.

$$H = \begin{bmatrix}  \dfrac{\partial^2 f}{\partial x_1^2} & \dfrac{\partial^2 f}{\partial x_2\,\partial x_1} & \cdots & \dfrac{\partial^2 f}{\partial x_N\,\partial x_1} \\\  \dfrac{\partial^2 f}{\partial x_1\,\partial x_2} & \dfrac{\partial^2 f}{\partial x_2^2} & \cdots & \dfrac{\partial^2 f}{\partial x_N\,\partial x_2} \\\  \vdots & \vdots & \ddots & \vdots \\\  \dfrac{\partial^2 f}{\partial x_1\,\partial x_N} & \dfrac{\partial^2 f}{\partial x_2\,\partial x_N} & \cdots & \dfrac{\partial^2 f}{\partial x_N^2}\end{bmatrix}$$



ML/DL에서 쓰이는 SGD, Momemtum, 네스테로프, AdaGrad, RMSProp, Adam 등은 모두 자코비안(1차 편미분)에만 의존한다. 헤시안 기반의 최적화 알고리즘들(CG & BFGS 등)이 있지만 이런 알고리즘들은 DNN에 적용하기에는 어렵다. 하나의 출력마다 $n^2$ 개의 2차 편미분을 계산해야 하기 때문이다. 수만, 수십만 개의 파라미터에 대해 헤시안 계산을 적용하기에는 메모리 용량이 한계를 가지고 또 너무 느리다.