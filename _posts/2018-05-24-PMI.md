---
title: PMI
date: 2018-05-24 02:09:46
categories:
- NLP
tags:
- nlp
---

## PMI (Point Mutual Information)

PMI는 BOW models 같은 sparse vector representation에서 semantics를 학습하기 위해 이용되는 방법이다. 



### 독립/상관관계

확률 이론에서 전체 공간에서의 $p(y)$ 와 $x$ 조건에서의 $p(y\vert x)$ 가 같으면 $x$ 와 $y$ 는 독립이다.

$$p(x, y) = p(x)p(y)$$

$$\dfrac{p(x, y)}{p(x) \times p(y)} = \dfrac{p(y\vert x)}{p(y)} = 1$$



$$\dfrac{p(안경, 저녁 )}{p(안경) \times p(저녁)} = \dfrac{1/12}{3/12 \times 4/12} = 1$$

|                    | 저녁을 먹었다 | 저녁을 먹지 않았다 | Prob. |
| ------------------ | ------------- | ------------------ | ----- |
| 안경을 썼다        | 100           | 200                | 3/12  |
| 안경을 쓰지 않았다 | 300           | 600                | 9/12  |
| Prob.              | 4/12          | 8/12               |       |

서로 양의 상관성이 있으면 $\dfrac{p(x, y)}{p(x) \times p(y)}$ 가 1보다 크다.

$$\dfrac{p(안경, 저녁 )}{p(안경) \times p(저녁)} = \dfrac{2/12}{5/12 \times 3/12} = 1.2$$

|                    | 저녁을 먹었다 | 저녁을 먹지 않았다 | Prob. |
| ------------------ | ------------- | ------------------ | ----- |
| 안경을 썼다        | 200           | 100                | 3/12  |
| 안경을 쓰지 않았다 | 300           | 600                | 9/12  |
| Prob.              | 5/12          | 7/12               |       |

PMI는 두 경우의 상관성을 파악하는 인덱스이다.

$$PMI(x, y) = \log \dfrac{p(x, y)}{p(x)\times p(y)}$$

- 양의 상관관계라면 0보다 큰 값을, 그 반대의 경우엔 0보다 작은 값을 가진다.
- 값의 방향성에 해석력이 있다.



### PPMI (Positive PMI)

그러나 자연어처리에서의 semantic에서는 음의 상관관계에 큰 의미가 없다. 따라서 양의 상관관계 패턴을 강조하기 위해 0보다 작은 값을 0으로 변환한다.

$$PPMI(x, y) = \text{max}(0, PMI(x, y))$$



### Smoothing PMI

PMI (PPMI)는 적은 빈도수에 민감하다. $p(y)$ 가 지나치게 작으면 대부분의 $y$ 가 $x$ 에서 발생할 가능성이 있다. 이를 방지하기 위해 $p(y)$ 에 일정한 값 $\alpha$ 를 더한다. $\alpha$ 는 $x$ 에 따라 다르게 적용되어야 한다.

- $p(y\vert x)$ 가 $\alpha$ 이상인 경우에만 PMI를 계산하는 효과가 있다.
- $\alpha$ 는 threshold 역할을 한다.

$$PMI(x, y) = \log \dfrac{p(x, y)}{p(x) \times (p(y) + \alpha)} = \log \dfrac{p(y\vert x)}{p(y) + \alpha}$$



### Defining contexts

semantic을 표현할 대상과 이를 설명하는 정보들을 설정한다. 

- $x$ 는 semantic을 표현할 대상이다.
- $y$ 는 $x$ 를 설명하는 정보 (context)이다.
- (Term, context term)을 ($x, y$)로 표현할 수 있다.

(word - context) pair는 Word2Vec의 개념과 유사하다. Levy & Goldberg (2014, NIPS)에서 Word2Vec은 PMI 행렬에 차원 축소 기법을 적용한 것과 비슷하다는 사실을 밝혔다.