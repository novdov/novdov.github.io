---
title: '[스파크] 02. RDD API 01 - Transformation (Map)'
description: 스파크 RDD의 기본 API를 정리합니다.
date: 2019-07-05 20:00:00
categories:
- Data Engineering
- Spark
tags:
- data engineering
- spark
---

> 참고: 스파크2 프로그래밍 (위키북스, 백성민)
>



# RDD API

아래는 참고하는 책에서 소개하는 RDD API를 정리합니다. 예제 코드는 스칼라 기준이며, SparkContext 생성과 실제 코드 작성 시의 `object` 선언 부분은 제외하고 실제 동작부 메서드만 표기했습니다.



## 기본 Action

### collect

`collect()`는 모든 원소를 모아 배열로 리턴합니다. `collect` 연산을 수행하면 RDD의 모든 요소들이 `collect` 연산을 호출한 서버의 메모리에 저장됩니다. 따라서 전체 데이터를 모두 담을 수 있을 정도의 충분한 메모리 공간이 확보되어 있는 상태에서만 사용해야 합니다.

```scala
def doCollect(sc: SparkContext): Unit = {
    val rdd = sc.parallelize(1 to 10)
    val result = rdd.collect()
    println(result.mkString(", "))
  }

>>> 
1, 2, 3, 4, 5, 6, 7, 8, 9, 10
```



### count

`count()`는 RDD를 구성하는 전체 요소의 개수를 리턴합니다.

```scala
def doCount(sc: SparkContext): Unit = {
    val rdd = sc.parallelize(1 to 10)
    val result = rdd.count()
    println(result)
  }

>>> 
10
```



## Transformation

Transformation은 기존 RDD를 이용해 새로운 RDD를 만드는 연산입니다. 크게는 다음으로 분류해볼 수 있습니다.

- Map 연산: 요소 간의 mapping을 정의한 함수를 RDD에 속하는 모든 요소에 적용해 새로운 RDD를 생성합니다.
- 그룹화 연산: 특정 조건에 따라 요소를 그룹화하거나 특정 함수를 적용합니다.
- 집합 연산: RDD에 포함된 요소를 하나의 집합으로 간주할 때 서로 다른 RDD 간에 합집합, 교집합 등을 계산합니다.
- 파티션 연산: RDD의 파티션 개수를 조정합니다.
- 필터와 정렬: 특정 조건을 만족하는 요소만 선택하거나 각 요소를 정해진 기준에 따라 정렬합니다.

한편 RDD가 제공하는 연산은 RDD를 구성하고 있는 데이터 유형과 밀접한 관계를 맺습니다. 특별한 설명이나 표시가 없다면 사용 가능한 데이터 타입에 특별한 제약이 없다는 뜻입니다.



###map

`map()` 메서드는 하나의 입력을 받아 하나의 값을 돌려주는 함수를 인자로 받은 뒤 이 함수를 RDD에 속하는 모든 요소에 적용한 뒤 그 결과로 구성된 새로운 RDD를 생성해 돌려줍니다.

```scala
def doMap(sc: SparkContext): Unit = {
    val rdd = sc.parallelize(1 to 5)
    val result = rdd.map(_ + 1)
    println(result.collect().mkString(", "))
  }

>>> 
2, 3, 4, 5, 6
```



### flatMap

`flatMap()` 은 `map()` 과 유사하지만 리턴 타입이 다릅니다. `flatMap()` 에 인자로 전달되는 함수는 스칼라의 iterator 타입 중 하나인 `TraversableOnce[U]` 타입을 리턴해야합니다.

아래의 예제에서는 과일 이름을 따로 추출해서 각 과일 이름 문자열로 구성된 RDD를 새로 만듭니다.

```scala
def doflatMap(sc: SparkContext): Unit = {
    val fruits = List("apple,orange", "grape,apple,mango", "blueberry,tomato,orange")
    val rdd1 = sc.parallelize(fruits)
    val rdd2 = rdd1.flatMap(_.split(","))
    println(rdd2.collect().mkString(", "))
  }

>>> 
apple, orange, grape, apple, mango, blueberry, tomato, orange
```

만약 위의 예제에서 `map()` 을 사용하면 문자열로 구성된 RDD가 아니라 문자열의 배열로 구성된 RDD를 얻습니다. (이는 우리가 원하는 결과가 아닙니다.)

`flatMap()` 을 이용하면 입력에 대응되는 출력이 여러 개인 경우 손쉽게 작업을 할 수 있습니다.



### mapPartitions

`mapPartitions()` 는 `map()` 과 `flatMap()` 과 달리 RDD를 파티션 단위로 처리합니다. 즉 인자로 전달받은 함수를 파티션 단위로 적용하고 그 결과로 구성된 새로운 RDD를 생성하는 메서드입니다.

파티션에 속한 모든 요소를 한번의 함수 호출로 처리할 수 있기 때문에 파티션 단위의 중간 산출물을 만들거나, 데이터베이스 연결 같은 고비용 자원을 파티션 단위로 공유해 사용할 수 있는 장점이 있습니다.

```scala
def doMapPartitions(sc: SparkContext): Unit = {
    val rdd1 = sc.parallelize(1 to 10, 3)
    val rdd2 = rdd1.mapPartitions(numbers => {
      println("Connect to DB !!!")
      numbers.map {
        number => number + 1
      }
    })
    println(rdd2.collect.mkString(", "))
  }

>>>
Connect to DB !!!
Connect to DB !!!
Connect to DB !!!
2, 3, 4, 5, 6, 7, 8, 9, 10, 11
```

먼저 3개의 파티션을 가진 `rdd1` 을 생성합니다. `mapPartitions()` 가 파티션 단위로 수행되어서 “Connect to DB !!!” 가 세 번 출력되었습니다. `numbers` 는 각 파티션의 요소를 가리키며 실제로는 각 파티션 내의 요소에 대한 이터레이터입니다. 따라서 `mapPartitions()` 에 전달되는 함수는 이터레이터를 전달받아 파티션의 각 요소에 대한 작업을 처리하고 그 결과를 다시 이터레이터 타입으로 리턴해야합니다. 즉 중간 결과가 하나의 정수 타입이더라도 정수 하나를 포함하고 있는 이터레이터 타입을 리턴해야 합니다.



### mapPartitionsWithIndex

`mapPartitionsWithIndex()` 는 `mapPartitions()` 와 비슷하지만 인자로 받는 함수를 호출할 때 파티션에 속한 요소의 정보뿐만 아니라 해당 파티션의 인덱스 정보도 함께 전달해 준다는 점입니다. 다음 예제는 첫 번째 파티션에서만 결과를 추출합니다.

```scala
def doMapPartitionsWithIndex(sc: SparkContext): Unit = {
    val rdd1 = sc.parallelize(1 to 10, 3)
    val rdd2 = rdd1.mapPartitionsWithIndex((idx, numbers) => {
      numbers.flatMap {
        case number if idx == 1 => Option(number + 1)
        case _ => None
      }
    })
    println(rdd2.collect.mkString(", "))
  }

>>>
5, 6, 7
```



### mapValues

RDD의 요소가 키/값 쌍을 이루고 있는 경우 PairRDD 라고 부릅니다. PairRDD에 속하는 데이터는 키를 기준으로 작은 그룹들을 만들고 해당 그룹들에 속한 값을 대상으로 함이나 평균을 구하는 등의 연산을 수행하는 경우가 많습니다. 스파크에서는 이런 경우를 위한 특별한 연산을 제공하고 있습니다.

`mapValues()` 는 RDD의 모든 요소들이 키/값 쌍을 이루고 있는 경우에만 사용 가능한 메서드입니다. 이 메서드는 인자로 전달받은 함수를 값에 해당하는 요소에만 적용하고 그 결과로 구성된 새로운 RDD를 생성합니다. 즉 메서드 이른 그래도 값 (values)에만 `map()` 연산을 적용한 것과 같습니다.

```scala
def doMapValues(sc: SparkContext): Unit = {
    val rdd = sc.parallelize(List("a", "b", "c")).map((_, 1))
    val result = rdd.mapValues(i => i + 1)
    println(result.collect.mkString("\t"))
  }

>>> 
(a,2)	(b,2)	(c,2)
```



### flatMapValues

`flatMapValues()` 역시 값에만 `flatMap()` 을 적용한 것과 같습니다.

```scala
def doFlatMapValues(sc: SparkContext): Unit = {
    val rdd = sc.parallelize(Seq((1, "a,b"), (2, "a,c"), (1, "d,e")))
    val result = rdd.flatMapValues(_.split(","))
    println(result.collect.mkString("\t"))
  }

>>>
(1,a)	(1,b)	(2,a)	(2,c)	(1,d)	(1,e)
```



